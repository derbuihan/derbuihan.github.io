[{"content":"はじめに 久々にCoqで遊びたくなったので、M1 Macにインストールしてみた。 最近のCoqはVSCodeで書くのが主流？らしいが、今回はEmacs + Proof-Generalで環境構築を行う。\n環境構築の方法 基本的にbrewやmelpaなどのpackage管理システムになるべく依存した形で環境構築を行う。\nはじめに、Coqのインストール(\u0026ndash;build-from-sourceをつけないとcoqtop実行時にエラーが出た。)\nbrew install coq --build-from-source GUI版Emacsのインストール\nbrew install emacs --cask Emacsの設定 次はEmacsの設定を行う。 Emacsの設定ファイルは~/.emacs.d/init.elに記載する。\nまずはmelpa(Emacsのプラグインのパッケージマネージャー)のセットアップを行う。 ~/.emacs.d/init.elに以下を記載し、\n(require 'package) (let* ((no-ssl (and (memq system-type '(windows-nt ms-dos)) (not (gnutls-available-p)))) (proto (if no-ssl \u0026quot;http\u0026quot; \u0026quot;https\u0026quot;))) (add-to-list 'package-archives (cons \u0026quot;melpa\u0026quot; (concat proto \u0026quot;://melpa.org/packages/\u0026quot;)) t)) (package-initialize) emacsを起動して\nM-x package-refresh-contents RET ←初回のみ実行\nを実行する。\n次に、~/.emacs.d/init.elに以下を記載し、\n(setq proof-splash-enable nil) (custom-set-variables '(coq-prog-name \u0026quot;/opt/homebrew/bin/coqtop\u0026quot;) '(package-selected-packages (quote (company-coq company proof-general))) '(proof-three-window-enable t)) (add-hook 'coq-mode-hook #'company-coq-mode) emacsを起動して\nM-x package-install-selected-packages RET\nを実行する。 これで環境構築が完了。\nおまけ ssreflectのインストール\nbrew install ssreflect 参考: SSReflectノート\nProofGeneralで使うキーバインド\nキー 動作 C-c C-n 次の.まで進む C-c C-u 次の.まで進む C-c C-Enter カーソル位置まで進める 参考: ProofGeneral/Keybinds\n感想 CoqIDEもいいけど、Emacs + ProofGeneral + company-coqをやると最強になる。 ","date":"2022-10-23","permalink":"https://derbuihan.github.io/posts/coq_install/","tags":["Coq","Emacs","環境構築"],"title":"Proof-Generalを用いたCoqの環境構築"},{"content":"はじめに Linuxカーネルを書き換えてTCPの再送時間のパレート最適性を裏切る方法はこの記事で実験を行った。 ただ私のようなMacユーザーはLinuxをサーバー用途でしか使わないため、カーネルを書き換えたところでそこまで得できない。 やはりこれを使うにはクライアント端末でKernelを書き換える方が楽しい。 私が普段使ってるクライアント端末でKernelを書き換えられそうなもの、そうAndroidである。 この記事では自分の持ってるスマホに、私が書き換えたkernelをインストールして実験してみる。 （この記事はただの検証記事でありTCPのプロトコルを裏切ることを推奨するものではありません。私も書き換えたカーネルを普段使いしているわけではありません。）\nAndroidをソースからビルドする方法 この記事ではPixelExperienceというAndroidのCustom firmwareのビルド方法を紹介する。 ビルドに用いたマシンは、OS: Ubuntu 22.04 LTS, メモリ: 16GB (+ Swap 32GB)である。 以下の内容は、この記事を参考にした。\nfastbootやadbやgitやpythonをインストールする。\nsudo apt install android-sdk git python-is-python3 androidをビルドするための依存関係全部インストールするスクリプト実行する。\ngit clone https://github.com/akhilnarang/scripts cd scripts ./setup/android_build_env.sh ビルド用のディレクトリを作る。\nmkdir -p ~/android/pe gitの設定を作る。（すでにやってたら必要なし）\ngit config --global user.email \u0026quot;you@example.com\u0026quot; git config --global user.name \u0026quot;Your Name\u0026quot; ~/android/peをrepositoryとして初期化。\ncd ~/android/pe repo init -u https://github.com/PixelExperience/manifest -b twelve-plus ソースをすべてダウンロード\nrepo sync -c -j$(nproc --all) --force-sync --no-clone-bundle --no-tags ディバイス固有のソースファイルをすべてダウンロード\nsource build/envsetup.sh lunch aosp_sweet-userdebug ccacheを用いてビルドをキャッシュする。 .bashrcに書き込む。\nexport USE_CCACHE=1 export CCACHE_EXEC=/usr/bin/ccache export CCACHE_DIR=~/.ccache ccacheの容量を指定し、圧縮を有効化。\nccache -C ccache -M 50G ccache -o compression=true あとはビルド\ncroot mka bacon -j$(nproc --all) boot.imgのみビルド\nmka bootimage -j$(nproc --all) echo $OUTにビルドしたinstaller packageや、boot.imgがあるはずなのでそれをtwrpなどで端末に突っ込んだら終了。 ちなみに、kernelを書き換えるためには、boot.imgのみをインストールすれば良い。\nadb, fastbootのコマンド集 ここでadbとfastbootでよく使うコマンドをまとめておく\nまず、端末がadbの命令を受け取れる状態かを確認する。\nadb devices 再起動、リカバリーモード、fastbootモードを起動\nadb reboot adb reboot recovery adb reboot fastboot ファイルを転送（例えばboot.imgを移す方法）\nadb push out/target/product/sweet/boot.img /sdcard/ Android内のShellを叩く（内部的にはtoyboxなので最低限のコマンドしか叩けない）\nadb shell ls fastbootモードでtwrpを起動（起動のみで書き込まれない。twrpの画面からflash出来る。）\nfastboot boot twrp-3.6.2_9-0-tissot.img これだけ覚えとけば大丈夫\nkernelをビルドしてインストール まず、~/android/pe/kernel/vender/device/内にカーネルのソースコードがあるので、以下に対応する部分を書き換える。\nhttps://github.com/torvalds/linux/blob/v5.15/include/net/tcp.h#L139\n変更前\n#define TCP_RTO_MAX\t((unsigned)(120*HZ)) 変更後\n#define TCP_RTO_MAX\t((unsigned)(1*HZ)) ビルド\nmka bootimage -j$(nproc --all) ファイルを転送（例えばboot.imgを移す方法）\nadb push out/target/product/sweet/boot.img /sdcard/ adb reboot recoveryでtwrpの画面からkernelをインストールする。\n検証 さて、192.168.0.20に23番ポートを塞いだサーバーを用意する。 このサーバーに対してnetcatを用いてtcpの通信を試みる。 具体的には以下のコマンドで計測を行った。\nadb shell time netcat 192.168.0.20 23 kernelの書き換え前と書き換え後でtime outまでの時間を計測した。\nkernelの書き換え前\n$ adb shell time netcat 192.168.0.20 23 netcat: connect: Connection timed out 0m31.26s real 0m00.01s user 0m00.01s system kernelの書き換え後\n$ adb shell time netcat 192.168.0.20 23 netcat: connect: Connection timed out 0m05.09s real 0m00.00s user 0m00.01s system 前回の実験と合わせて、Androidはnet.ipv4.tcp_syn_retries = 4に設定していることがわかる。 その上で再送の間隔が指数関数的に伸びないように変更出来た。\n感想 Androidのビルドしてインストールは以外と簡単に出来た。（数時間CPU使用率100%だったが。） Androidのlinux kernelは普通のlinux kernelと結構似ているので意外と遊びの幅が広いかもしれない。 古いAndroid端末向けに最新Androidをビルドするとかやってみたい。 ","date":"2022-10-21","permalink":"https://derbuihan.github.io/posts/android_tcp_rto_hack/","tags":["Linux","TCP","kernel"],"title":"Linuxカーネルを書き換えてTCPの再送時間のパレート最適性を裏切る方法 〜Android編〜"},{"content":"はじめに 私はブラウザ上で動く簡単なアプリを作る際にelmを用いることが多い。 elmはブラウザの状態とユーザーの入力を言語の機能とそのアーキテクチャで簡単に管理できるため、簡単なアプリであればほとんど迷うことがなく作りたいものが作れる点で気にいっている。 ただ最近では、elmの便利な状態管理の手法はJSのフレームワークにも導入され、Reactのコンポーネントを用いたUI開発やJSのその他の資源と組み合わせることで効率的にフロントエンドを開発することが主流になってきている。 このような流れのためか、ここ数年はelmを用いた開発を行う人が極端に少なくなってきていると感じている。 ただフロントエンドの専門家ではない私からすると、フレームワークの選定に迷う必要がない点や、tsにするかjsにするか迷わなくて良い点や、Haskellライクな言語の楽さとelm-formatの利便性などの観点から考えると、簡単なWebアプリであれば依然としてelmを用いて開発を行うことは悪くない選択肢だと思っている。 この記事ではelmとBootstrapやelmとTailwind CSSを組み合わせたサンプルアプリの構築方法を解説する。\n環境構築(Bootstrap編) プロジェクトを作成\n$ mkdir elm-bootstrap-sampleapp $ cd elm-bootstrap-sampleapp $ npm init -y bootstrapをインストール（参考）\n$ npm i --save-dev parcel $ npm i --save-dev bootstrap @popperjs/core elmをインストールして環境を構築\n$ npm i --save-dev elm @parcel/transformer-elm $ npx elm init 必要なファイルを作成\n$ touch src/index.html src/index.js src/index.scss src/Main.elm src/index.htmlを書く。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1.0\u0026quot;\u0026gt; \u0026lt;link href=\u0026quot;index.scss\u0026quot; rel=\u0026quot;stylesheet\u0026quot;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026quot;root\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026quot;module\u0026quot; src=\u0026quot;index.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; src/index.jsを書く。 bootstrapとelmのjavascriptが共存している。\nimport * as bootstrap from \u0026quot;bootstrap\u0026quot;; import { Elm } from \u0026quot;./Main.elm\u0026quot;; Elm.Main.init({ node: document.getElementById(\u0026quot;root\u0026quot;) }); src/index.scssを書く。\n@import \u0026quot;bootstrap/scss/bootstrap\u0026quot;; src/Main.elmを書く。\nmodule Main exposing (main) import Browser import Html exposing (Html, button, div, text) import Html.Attributes exposing (class) import Html.Events exposing (onClick) main = Browser.sandbox { init = init, update = update, view = view } type alias Model = { score : Int } init : Model init = { score = 0 } type Msg = Select String update : Msg -\u0026gt; Model -\u0026gt; Model update msg model = case msg of Select _ -\u0026gt; { model | score = model.score + 1 } view : Model -\u0026gt; Html Msg view model = div [] [ button [ class \u0026quot;btn btn-primary\u0026quot;, onClick (Select \u0026quot;a\u0026quot;) ] [ text (String.fromInt model.score) ] ] package.jsonを書く。\n{ \u0026quot;name\u0026quot;: \u0026quot;elm-bootstrap-sampleapp\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;main\u0026quot;: \u0026quot;index.js\u0026quot;, \u0026quot;scripts\u0026quot;: { \u0026quot;start\u0026quot;: \u0026quot;parcel src/index.html\u0026quot;, \u0026quot;build\u0026quot;: \u0026quot;parcel build src/index.html --no-source-maps\u0026quot;, \u0026quot;test\u0026quot;: \u0026quot;echo \\\u0026quot;Error: no test specified\\\u0026quot; \u0026amp;\u0026amp; exit 1\u0026quot; }, \u0026quot;keywords\u0026quot;: [], \u0026quot;author\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;license\u0026quot;: \u0026quot;ISC\u0026quot;, \u0026quot;devDependencies\u0026quot;: { \u0026quot;@parcel/transformer-elm\u0026quot;: \u0026quot;^2.7.0\u0026quot;, \u0026quot;@parcel/transformer-sass\u0026quot;: \u0026quot;^2.7.0\u0026quot;, \u0026quot;@popperjs/core\u0026quot;: \u0026quot;^2.11.6\u0026quot;, \u0026quot;bootstrap\u0026quot;: \u0026quot;^5.2.2\u0026quot;, \u0026quot;elm\u0026quot;: \u0026quot;^0.19.1-5\u0026quot;, \u0026quot;parcel\u0026quot;: \u0026quot;^2.7.0\u0026quot; } } npm run startで開発出来る。 npm run buildで圧縮してビルドが出来る。\n環境構築(Tailwind CSS編) プロジェクトを作成\n$ mkdir elm-tailwindcss-sampleapp $ cd elm-tailwindcss-sampleapp $ npm init -y elmとparcelとtailwindcssをインストール\n$ npm i -D elm parcel tailwindcss @parcel/transformer-elm elmとtailwindcssの環境を構築\n$ npx elm init $ npx tailwindcss init 必要なファイルを作成する。\n$ touch .postcssrc src/index.html src/index.css src/index.js src/Main.elm .postcssrcを作成してtailwindcssをコンパイル出来るようにする。\n{ \u0026quot;plugins\u0026quot;: { \u0026quot;tailwindcss\u0026quot;: {} } } tailwindcssが見るファイルにhtmlとjsとelmを設定する。\n/** @type {import('tailwindcss').Config} */ module.exports = { content: [ \u0026quot;./src/**/*.{html,js,elm}\u0026quot;, ], theme: { extend: {}, }, plugins: [], } package.jsonを書く。\n{ \u0026quot;name\u0026quot;: \u0026quot;elm-tailwindcss-sampleapp\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;scripts\u0026quot;: { \u0026quot;start\u0026quot;: \u0026quot;parcel src/index.html\u0026quot;, \u0026quot;build\u0026quot;: \u0026quot;parcel build src/index.html --no-source-maps\u0026quot;, \u0026quot;clean\u0026quot;: \u0026quot;rm dist/*\u0026quot;, \u0026quot;test\u0026quot;: \u0026quot;echo \\\u0026quot;Error: no test specified\\\u0026quot; \u0026amp;\u0026amp; exit 1\u0026quot; }, \u0026quot;keywords\u0026quot;: [], \u0026quot;author\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;license\u0026quot;: \u0026quot;ISC\u0026quot;, \u0026quot;devDependencies\u0026quot;: { \u0026quot;@parcel/transformer-elm\u0026quot;: \u0026quot;^2.7.0\u0026quot;, \u0026quot;elm\u0026quot;: \u0026quot;^0.19.1-5\u0026quot;, \u0026quot;parcel\u0026quot;: \u0026quot;^2.7.0\u0026quot;, \u0026quot;tailwindcss\u0026quot;: \u0026quot;^3.1.8\u0026quot; } } src/index.htmlを書く。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1.0\u0026quot;\u0026gt; \u0026lt;link href=\u0026quot;index.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026quot;root\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026quot;module\u0026quot; src=\u0026quot;index.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; src/index.cssを書く。\n@tailwind base; @tailwind components; @tailwind utilities; src/index.jsを書く。\nimport { Elm } from \u0026quot;./Main.elm\u0026quot;; Elm.Main.init({ node: document.getElementById(\u0026quot;root\u0026quot;) }); src/Main.elmを書く。\nmodule Main exposing (main) import Browser import Html exposing (Html, button, div, text) import Html.Attributes exposing (class) import Html.Events exposing (onClick) main = Browser.sandbox { init = init, update = update, view = view } type alias Model = { score : Int } init : Model init = { score = 0 } type Msg = Select String update : Msg -\u0026gt; Model -\u0026gt; Model update msg model = case msg of Select _ -\u0026gt; { model | score = model.score + 1 } view : Model -\u0026gt; Html Msg view model = div [] [ button [ class \u0026quot;rounded-full bg-gray-300 px-4 py-2\u0026quot;, onClick (Select \u0026quot;a\u0026quot;) ] [ text (String.fromInt model.score) ] ] npm run startで開発出来る。 npm run buildで圧縮してビルドが出来る。\n感想 Bootstrapはjsに依存しているのでelmと組み合わせて使って良いものなのかよくわからない。とりあえずはうまく動いている。一方でpureCSSのTailwind CSSはcssが書ける人用のフレームワークのため、cssが得意でない私では使いこなせない代物だと思う。私は楽したいのでBootstrapが好みかなぁ。 Reactもtsと状態管理フレームワークと組み合わせればelm以上に開発効率が良いのだろうなぁと思う。elmのHaskellライクな言語が好きなのでなかなかそちらに行けないが。 ","date":"2022-10-17","permalink":"https://derbuihan.github.io/posts/elm_sampleapp/","tags":["Elm","Bootstrap","TailwindCSS"],"title":"elmとBootstrapやTailwind CSSを組み合わせて簡単なWebサービスを開発する方法"},{"content":"はじめに インターネットには多数のデバイスが存在しており、その多数のデバイスが通信し合うために通信プロトコルが定義されている。 様々な通信プロトコルを勉強していくと、ネットワーク内の通信が正常に行えるためにはそのネットワークの参加者全員が通信プロトコルを守っていることが必要になることに気がつく。\nたとえば、TCPの通信が失敗したときの再送の間隔について説明する。 今回はわかりやすく普通のサーバー・クライアントモデルのWebサービスを考え、サーバー対してクライアントから大量のアクセスが集まりそのリソースが枯渇した状況を考える。 サーバーはリソースの枯渇により全てのクライアントとTCPのコネクションを張れないことから、一部クライアントからのアクセスを拒否する。 拒否されたクライアントは再度サーバーに対してアクセスしようと試みる。 これをアクセス出来るまでクライアントは繰り返すわけだが、その間隔は実は一定ではない。 最初は1秒程度で素早く再送するが、サーバーが何度もアクセスを拒否すると、クライアントの方で自動で再送の間隔を指数関数的に伸ばしてアクセスするようになっている。 このような仕様になってる理由は、全員が間隔を開けずに即座に繰り返し通信を試みた場合はサーバーのリソースは一生枯渇したままで回復しない可能性があるためである。 一方でクライアント側で指数関数的に間隔を伸ばしていくと、サーバーがアクセスを拒否し続ければいつかはリソースが回復し少しクライアントの通信を捌けるようになる。 そのため、TCPではアクセスに失敗したときクライアント側で再送する間隔を指数関数的に引き伸ばしていくというプロトコルになっているのである。\nここで注目したいのが、TCPの再送間隔の調整はクライアントで行われるという点である。 再送の間隔を極端に短くして再度送信するようなデバイスを作ったとしたら、そのデバイスはネットワークの中で唯一得することが出来るのである。 囚人のジレンマの言葉で言えば、現在のネットワークは全員が黙秘しているパレート最適な状態であるから、自分だけ自白することで自分だけの利得を最大化出来るのである。 (世界中の数百億台というディバイスでパレート最適な状態を保っているというのは考え深いものがある。) このようなインターネットのプロトコルのパレート最適性を裏切る方法はTCPの再送時間以外にも探せばいくつもあるだろう。 この記事ではTCPの再送間隔についてLinuxカーネルを弄って実験をする。\n実験条件 OS: Ubuntu 22.04.1 LTS Linux kernel version: 5.15.73\nServerとClientのIPアドレスは\nServer: 192.168.0.10 Client: 192.168.0.20 とする。\nTCPがアクセスを再試行する回数は/etc/sysctl.conf内に、\nnet.ipv4.tcp_syn_retries = 5 を書くことで調整が可能である。（この場合は5回リトライする）\nつぎに、TCPの再送時間の計測方法を説明する。 まずServerで23番のTCPポートを塞ぐ。 クライアント側でServerにtelnetでアクセスを行う。\n$ time telnet 192.168.0.10 Trying 192.168.0.10... telnet: Unable to connect to remote host: Connection timed out real 1m4.605s user 0m0.003s sys 0m0.000s 今回は、5回のリトライで1分4.6秒かかったことがわかる。\nそのままのカーネルで再送間隔を計測 以上のやり方で再送回数と時間の計測を行った。\n再送回数 時間 1 3.041s 2 7.266s 3 15.404s 4 31.399s 5 1m4.605s 指数関数的に増大していることがわかる。 さて次はLinux kernelの書き換える方法を説明し再送時間を弄ってみます。\nカーネルのビルド方法 まず、以下からカーネルのソースを取ってくる。\nhttps://www.kernel.org/\n私が取ってきたのはlinux-5.15.73.tar.xz。\n解凍\n$ tar xfv linux-5.15.73.tar.xz Ubuntuのkernelのconfigを取ってくる。 make localmodconfigはエンター連打。\n$ cp /boot/config-5.15.0-50-generic .config $ make localmodconfig 適当にccacheをリセットする。\n$ ccache -Cz Linux kernelをビルドしてインストール\n$ ccache make -j8 $ make modules_install $ make install 再起動すればソースからビルドされてLinux kernelを使えるようになる。\nLinuxカーネルの書き換え 次に、Linuxカーネルを書き換えて再送間隔を短くする。\nLinuxカーネルをどこを書き換えるかが問題になるが、結論から言うと以下の行を1行書き換えるだけである。\nhttps://github.com/torvalds/linux/blob/v5.15/include/net/tcp.h#L139\n変更前\n#define TCP_RTO_MAX\t((unsigned)(120*HZ)) 変更後\n#define TCP_RTO_MAX\t((unsigned)(1*HZ)) TCP_RTO_MAXは再送間隔の最大値を指定しており、指数関数的に大きくなると言ってもどこかで切らなきゃいけないわけで、その切る値を定義していると解釈している。 わたしもTCPのコードの全てを理解したわけではなく、ただこの変数が怪しかったので書き換えてみたら再送間隔が短くなった程度の理解である。 またこれを見つけるまでにはこのブログ記事が大変参考になった。\n弄ったカーネルで再送間隔を計測 以上のやり方で再送回数と時間の計測を行った。\n再送回数 時間 1 2.036s 2 3.079s 3 4.091s 4 5.118s 5 6.148s 再送間隔が等差数列になった。 つまりTCPの再送間隔の裏切りに成功したのである。\nまとめ ネットワークプロトコルをゲーム理論的視点から考察してみると面白いと誰かから聞いたので一番簡単そうなTCPの再送間隔で実験してみた。 wikipediaのTCPの記事は相当詳しくてLinux kernelのソースを見て書いたんだなぁとか、Linux kernelの細かいコメントが意外と色々書いてあることとかわかった。 Linux kernelのビルドは久々にやった。ccache使うと早くビルド出来るのは良いが、デバッグするたびにPC再起動するのがめんどくさい。再起動せずに開発するやり方あるのかな。 その他のプロトコルについても、パレート最適に落ち着いてて裏切ると自分だけ得できるパターンがあるので、kernelの書き換え場所を見つけ次第記事にしたい。 ","date":"2022-09-16","permalink":"https://derbuihan.github.io/posts/linux_tcp_rto_hack/","tags":["Linux","TCP","kernel"],"title":"Linuxカーネルを書き換えてTCPの再送時間のパレート最適性を裏切る方法"},{"content":"Basic認証やDigest認証をゼロから実装する機会があったのでその仕組をメモしておく。 本記事のnodeはすべてv18である。\nBasic認証とDigest認証 Basic認証 はじめにBasic認証の仕組みにを解説する。\nBasic認証においてクライアントが初めてhttpリクエストを送ったときは、Status Codeが401 Unauthorizedでヘッダーに\nwww-authenticate: Basic realm=\u0026quot;secure\u0026quot; が付与されたレスポンスがサーバーから帰ってくる。 ちなみにrealmは認証領域を表す。\nクライアントがユーザーとパスワードをuser:passのように入力したときは\nAuthorization: Basic dXNlcjpwYXNz を付与してレスポンスを返す仕組みになっている。 このような動作はBasic認証を用いたサイトにChromeで接続し、デベロッパーツールで通信を監視することで確かめることが出来る。\n次に、dXNlcjpwYXNzの計算方法について確認しておく。 結論から言うとdXNlcjpwYXNzはuser:passをbase64でエンコードしたものである。 Bashでは\n$ echo -n \u0026quot;user:pass\u0026quot; | base64 dXNlcjpwYXNz JavaScriptでは\n\u0026gt; btoa(\u0026quot;user:pass\u0026quot;) 'dXNlcjpwYXNz' のように計算出来る。\nただ、Base64はハッシュ関数でもなんでもなくて文字列を単純な方法で変換しているだけなので、簡単に戻すことが出来る。 Bashでは\n❯ echo -n \u0026quot;dXNlcjpwYXNz\u0026quot; | base64 -d user:pass JavaScriptでは\n\u0026gt; atob(\u0026quot;dXNlcjpwYXNz\u0026quot;) 'user:pass' のように計算出来る。 これはBasic認証において通信経路でhttpのヘッダーを覗かれるとユーザー名もパスワードもすべて筒抜けになることを示している。 つまりBasic認証を使う際はhttpsなどを用いて暗号化するなど対策が必要になる。\nDigest認証 次にDigest認証について説明を行う。 Basic認証の欠点である通信経路をユーザー名とパスワードがそのまま流れてしまうという欠点を補ったのがDigest認証である。\nDigest認証においてクライアントがサーバーにhttpリクエストを送ったときは、Status Codeが401 Unauthorizedでヘッダーに\nwww-authenticate: Digest realm=\u0026quot;secure\u0026quot;, nonce=\u0026quot;9HYa-V1Mba3QOk6f\u0026quot;, algorithm=MD5, qop=\u0026quot;auth\u0026quot; を付与したレスポンスがサーバーから帰ってくる。 realmは認証領域、nonceは認証情報を隠すための乱数、algorithmは用いるハッシュ関数のアルゴリズムを表している。\nクライアントがユーザーとパスワードをuser:passのように入力したときは\nauthorization: Digest username=\u0026quot;user\u0026quot;, realm=\u0026quot;secure\u0026quot;, nonce=\u0026quot;9HYa-V1Mba3QOk6f\u0026quot;, uri=\u0026quot;/\u0026quot;, algorithm=MD5, response=\u0026quot;f11fd3424129e2f27546ce0fc1fd995a\u0026quot;, qop=auth, nc=00000002, cnonce=\u0026quot;59f367b44815ac37\u0026quot; を付与してレスポンスを返す仕組みになっている。 ここで、uriはコンテンツのURI、responseはハッシュ値であり認証に用いられる値、ncは同じnonceを用いてリクエストするたびに増えてく値、cnonceはクライアント側で作られる乱数である。 responseはユーザー名とパスワードとnonceやcnonceなどを用いて作られる。 この値が毎回異なるハッシュ値のため、ここからユーザー名とパスワードを導くことは不可能である。\n次に、responseを計算する方法について説明する。 wikipediaにはresponseは\nA1 = ユーザ名 \u0026quot;:\u0026quot; realm \u0026quot;:\u0026quot; パスワード A2 = HTTPのメソッド \u0026quot;:\u0026quot; コンテンツのURI response = MD5( MD5(A1) \u0026quot;:\u0026quot; nonce \u0026quot;:\u0026quot; nc \u0026quot;:\u0026quot; cnonce \u0026quot;:\u0026quot; qop \u0026quot;:\u0026quot; MD5(A2) ) で計算されると書いてある。\nこれを参考にBashでresponseを計算すると、\n$ HA1=`echo -n \u0026quot;user:secure:pass\u0026quot; | md5` $ HA2=`echo -n \u0026quot;GET:/\u0026quot; | md5` $ nonce=9HYa-V1Mba3QOk6f $ nc=00000002 $ cnonce=59f367b44815ac37 $ qop=auth $ echo -n \u0026quot;${HA1}:${nonce}:${nc}:${cnonce}:${qop}:${HA2}\u0026quot; | md5 f11fd3424129e2f27546ce0fc1fd995a となる。\nJavaScriptでは\nconst crypto = require('crypto'); const md5 = async (data) =\u0026gt; { const md5 = crypto.createHash('md5'); return md5.update(data, 'binary').digest('hex') }; (async () =\u0026gt; { const username = \u0026quot;user\u0026quot;; const password = \u0026quot;pass\u0026quot;; const realm = \u0026quot;secure\u0026quot;; const HA1 = await md5([username, realm, password].join(\u0026quot;:\u0026quot;)); const method = \u0026quot;GET\u0026quot;; const uri = \u0026quot;/\u0026quot;; const HA2 = await md5([method, uri].join(\u0026quot;:\u0026quot;)); const nonce = \u0026quot;9HYa-V1Mba3QOk6f\u0026quot;; const nc = \u0026quot;00000002\u0026quot;; const cnonce = \u0026quot;59f367b44815ac37\u0026quot;; const qop = \u0026quot;auth\u0026quot;; const response = await md5([HA1, nonce, nc, cnonce, qop, HA2].join(\u0026quot;:\u0026quot;)); console.log(response); // f11fd3424129e2f27546ce0fc1fd995a })(); で計算できる。\nまとめ Digest認証のハッシュ値の計算をBashとJavaScriptを用いて計算を行った。 Digest認証はサーバー側で生成したnonceとnonceを使った回数であるncを状態を保持して管理しないと、httpのリクエストを盗聴された際にそのHeaderをコピーしてリクエストすれば認証を突破出来てしまうと思った。（クライアントからサーバーへの二回目のアクセスの際にnonceやncを付与して送信しているが、このnonceがきちんとサーバーから生成されたものであり、このnonceを用いたnc回目の通信が以前に行われていないことを確認する必要がある。） Basic認証はサーバー側で状態を保持しなくていい分、圧倒的に実装が楽だからよく利用されているんだなぁと思った。 Digest認証はサーバー側で状態を保持して毎回検索するので、メモリやCPUのリソースを結構食うのかもしれない。 Digest認証でもhttpsにしなければ中間者攻撃に弱いという話があるが、勝手にBasic認証に変えちゃう以外の攻撃方法ってあるのかな。サーバー側でnonceとncを管理しとけば、ヘッダー情報取られてもリクエスト1回しか認証突破できないし、その攻撃範囲もmethodとuriが限られる。実は意外とDigest認証って安全な気がするので、ブラウザでもBasic認証と表示を変えてくれればいいのに。 一つ懸念として、nonceとcnonceとresponseが同時に流れてくるのでこの情報を集めておいて、スパコンとか使って頑張って計算してHA1を割り出すことは可能なんじゃないかな？HA1さえ割り出してしまえばそれで認証突破は出来る気がする。cnonceをHA1に含めてハッシュ化とかしたらいいのにって思ってしまった。なんか勘違いしてるのかな。 追記 Digest認証において、リバースプロキシが状態を保持しているのか確認したかったので、ncが重複するリクエストを送りつける実験を行う。 今回は私が普段から使っているtraefikを用いて試験を行う。\nまずtraefikのドキュメントを確認しながら、http://localhostにtraefikを立ててdigest認証を有効化する。 そして次のコードを書く。\nconst crypto = require('crypto'); const KD = async (username, password, realm, method, uri, nonce, nc, cnonce, qop) =\u0026gt; { const HA1 = await md5([username, realm, password].join(\u0026quot;:\u0026quot;)); const HA2 = await md5([method, uri].join(\u0026quot;:\u0026quot;)); const response = await md5([HA1, nonce, nc, cnonce, qop, HA2].join(\u0026quot;:\u0026quot;)); return response }; const parse = (credentials) =\u0026gt; { if (credentials == null || credentials == undefined) return; let c = {}; credentials .substr(7) .split(\u0026quot;, \u0026quot;) .forEach((param) =\u0026gt; { const [key, val] = param.split(\u0026quot;=\u0026quot;); c[key] = val.replace(/\u0026quot;/g, \u0026quot;\u0026quot;); }); return [c.realm, c.nonce, c.opaque, c.algorithm, c.qop]; }; (async () =\u0026gt; { const username = \u0026quot;user\u0026quot;; const password = \u0026quot;pass\u0026quot;; const method = \u0026quot;GET\u0026quot;; const uri = \u0026quot;/\u0026quot;; // const nc = \u0026quot;00000002\u0026quot;; 二度目の実験ではこのコメントを外す const cnonce = \u0026quot;2b93895fec6ce68d\u0026quot;; const url = \u0026quot;http://localhost/\u0026quot;; const res1 = await fetch(url); const credentials = res1.headers.get(\u0026quot;www-authenticate\u0026quot;); console.log(res1.status); let [realm, nonce, opaque, algorithm, qop] = await parse(credentials); for (let i = 1; i \u0026lt; 10; i++) { const nc = ('00000000' + i).slice(-8); // 二度目の実験ではコメントアウト const response = await KD(username, password, realm, method, uri, nonce, nc, cnonce, qop); const options = { \u0026quot;method\u0026quot; : 'GET', \u0026quot;headers\u0026quot;: { \u0026quot;Authorization\u0026quot;: 'Digest ' + 'username=\u0026quot;' + username + '\u0026quot;, realm=\u0026quot;' + realm + '\u0026quot;, nonce=\u0026quot;' + nonce + '\u0026quot;, uri=\u0026quot;' + uri + '\u0026quot;, algorithm=' + algorithm + ', response=\u0026quot;' + response + '\u0026quot;, opaque=\u0026quot;' + opaque + '\u0026quot;, qop=' + qop + ', nc=' + nc + ', cnonce=\u0026quot;' + cnonce + '\u0026quot;' } }; const res2 = await fetch(url, options); console.log(res2.status); } })(); このコードをmain.jsで保存して実行すると、\n$ node main.js 401 200 200 200 200 200 200 200 200 200 となり、最初以外全て認証突破している。 一方で、二度目の実験(ncを変化させない)を行うと、\n$ node main.js 401 200 401 401 401 401 401 401 401 401 となる。 最初の401はサーバーがnonceを返す。 二度目の200はncが初めて使われるので認証突破。 それ移行はncがすでに使われているので認証を突破できない。 このことからtraefikのDigest認証は真面目に状態を保持していることがわかった。 さらに、サーバーで作ったnonceを無視して、クライアント側で勝手にnonceを作って認証情報を送るなどの実験もしたが、それでは突破出来なかった。 またcnonceは毎回同じでも認証には関係なかった。\ntraefikの実装を見てみると、digest認証の部分にはabbot/go-http-authが使われている。 このリポジトリを見ると内部でメモリのMapやArrayなどのデータ構造を用いてncやnonceを管理している様子。\n","date":"2022-07-13","permalink":"https://derbuihan.github.io/posts/digest_auth_bash_js/","tags":["DigestAuth","JS","Bash"],"title":"Digest認証の仕組みをBashとJavaScriptで理解したい。"},{"content":"1次元のイジング模型 周期的境界条件のもとで1次元のイジング模型のハミルトニアンは $$ H\\left(\\sigma_{1}, \\cdots, \\sigma_{N} \\right) = -J \\sum_{i} \\sigma_{i} \\sigma_{i+1} - h \\sum_{i} \\sigma_{i} $$ である。 ここで、$\\sigma_{i}=\\pm 1$はスピンを表し、周期的境界条件から$\\sigma_{1} = \\sigma_{N+1}$が成り立つ。 また$h$は外部磁場を表し、$J$は隣り合うスピンの相互作用を表す。\nこのハミルトニアン$H$から分配関数$Z$は $$ Z = \\sum_{\\sigma_{1}, \\cdots, \\sigma_{N} = \\pm 1} e^{- \\beta H \\left(\\sigma_{1}, \\cdots, \\sigma_{N} \\right)} $$ と計算出来る。 ここで、$\\beta = \\frac{1}{k_B T}$は逆温度である。 この分配関数は次のように書き換えることが出来る。 $$ Z = \\sum_{\\sigma_{1}, \\cdots, \\sigma_{N}} \\exp{\\left( \\sum_i \\beta J \\sigma_i \\sigma_{i+1} + \\frac{\\beta h}{2} (\\sigma_i + \\sigma_{i+1}) \\right)} = \\mathrm{tr} \\left( T^N \\right) $$ ここで、$T = \\begin{pmatrix} e^{\\beta(J+h)} \u0026amp; e^{-\\beta J} \\cr e^{-\\beta J} \u0026amp; e^{\\beta(J-h)} \\end{pmatrix}$である。 さらに、行列$T$を対角化: $T=P^{-1} \\Lambda P$, $\\Lambda = \\mathrm{diag}(\\lambda_1, \\lambda_2)$を計算すれば、分配関数は$Z = \\lambda_1^N + \\lambda_2^N$が計算出来る。\n分配関数$Z$を用いて\n自由エネルギー: $F = \\frac{-1}{\\beta} \\log Z$ 内部エネルギー: $E = - \\frac{\\partial \\log Z}{\\partial \\beta}$ 比熱: $C = \\frac{\\partial E}{\\partial T}$ 磁化: $M = \\frac{1}{\\beta} \\frac{\\partial \\log Z}{\\partial h}$ 磁化率: $\\chi = \\left. \\frac{\\partial M}{\\partial h} \\right|_{h=0}$ のように物理量を計算出来る。 1次元のイジングモデルの場合これらの量が厳密に計算出来ることが知られている。\nこの記事では、PyTorchで分配関数$Z$を計算し自動微分を用いて物理量の計算する。\nPyTorchを用いた分配関数の計算 Pytorchを用いると分配関数は次のように計算出来る。\nimport torch from torch import exp, pow, trace, log, tensor def partition_function(beta, h): J = 1.0 T = exp(beta * (tensor([[J, -J], [-J, J]]) + h * tensor([[1, 0], [0, -1]]))) Z = trace(pow(T, 10)) return Z t = tensor(1.0, requires_grad=True) beta = 1./t h = tensor(1., requires_grad=True) Z = partition_function(beta, h) 自由エネルギーは\nF = -log(Z) / beta で計算出来る。\n内部エネルギー$E$と磁化$M$は$\\log Z$の微分であるから、\nE = torch.autograd.grad(-log(Z), beta, create_graph=True)[0] M = torch.autograd.grad(log(Z), h, create_graph=True)[0] / beta で計算出来る。\n更に比熱$C$は内部エネルギー$E$の微分なので\nC = torch.autograd.grad(E, t, create_graph=True)[0] で計算出来る。\nまた、磁化率$\\chi$は$\\log Z$の$h$の二回微分を$h=0$における値であるから\nh = tensor(0., requires_grad=True) Z = partition_function(beta, h) M_ = torch.autograd.grad(log(Z), h, create_graph=True)[0] / beta x, = torch.autograd.grad(M_, h, create_graph=True) で計算出来る。\n以上のようにすべての物理量をPyTorchの自動微分の機能で計算はできる。\n物理量の温度変化を可視化 物理量の温度変化を計算する。 手計算でも行える計算を自動微分を用いて数値計算しているだけなので結果は正しいはず。\nimport torch from torch import exp, pow, trace, log, tensor import numpy as np import matplotlib.pyplot as plt def partition_function(beta, h): J = 1.0 T = exp(beta * (tensor([[J, -J], [-J, J]]) + h * tensor([[1, 0], [0, -1]]))) Z = trace(pow(T, 10)) return Z data = [] for i in range(1, 50): t = tensor(float(i), requires_grad=True) beta = 1. / t h = tensor(1., requires_grad=True) Z = partition_function(beta, h) F = -log(Z) / beta E, = torch.autograd.grad(-log(Z), beta, create_graph=True) C, = torch.autograd.grad(E, t, create_graph=True) M = torch.autograd.grad(log(Z), h, create_graph=True)[0] / beta h = tensor(0., requires_grad=True) Z = partition_function(beta, h) M_ = torch.autograd.grad(log(Z), h, create_graph=True)[0] / beta x, = torch.autograd.grad(M_, h, create_graph=True) data.append([t.item(), beta.item(), F.item(), E.item(), C.item(), M.item(), x.item()]) data = np.array(data).T 自由エネルギー: $F$ plt.title(\u0026quot;F / t\u0026quot;) plt.xlabel(\u0026quot;t\u0026quot;) plt.ylabel(\u0026quot;F\u0026quot;) plt.plot(data[0], data[2]) 内部エネルギー: $E$ plt.title(\u0026quot;E / t\u0026quot;) plt.xlabel(\u0026quot;t\u0026quot;) plt.ylabel(\u0026quot;E\u0026quot;) plt.plot(data[0], data[3]) 比熱: $C$ plt.title(\u0026quot;C / t\u0026quot;) plt.xlabel(\u0026quot;t\u0026quot;) plt.ylabel(\u0026quot;C\u0026quot;) plt.plot(data[0], data[4]) 磁化: $M$ plt.title(\u0026quot;M / t\u0026quot;) plt.xlabel(\u0026quot;t\u0026quot;) plt.ylabel(\u0026quot;M\u0026quot;) plt.plot(data[0], data[5]) 磁化率: $\\chi$ plt.title(\u0026quot;x / t\u0026quot;) plt.xlabel(\u0026quot;t\u0026quot;) plt.ylabel(\u0026quot;x\u0026quot;) plt.plot(data[0], data[6]) まとめ 自動微分を用いて分配関数を微分してみた。 今回は分配関数が微分可能演算のみで構成されているために、自動微分を用いて微分が出来た。さらに複雑なモデルでも分配関数が微分可能な計算のみで構成されていれば、物理量を計算が出来ると思う。 分配関数$Z=\\sum_i e^{-\\beta H_i}$は指数関数を大量に足し合わせる計算なので、実は簡単にFloatの限界に到達してしまう。なのでもっと大きなモデル($n=100$とか)を計算するにはパラメータをうまく調整してFloatに納める必要がある。 今回も自動微分ライブラリを機械学習以外の目的で利用して遊んでみた。他の遊び方も見つけ次第記事にしたい。 ","date":"2022-06-03","permalink":"https://derbuihan.github.io/posts/pytorch_1d_ising/","tags":["pytorch","Python"],"title":"PyTorchの自動微分を使った1次元のイジング模型の分配関数の計算"},{"content":"充足可能性問題 (satisfiability problem, SAT) 「与えられた論理式を真にする真偽値$x_1, x_2 \\dots$が存在するか？」という問題を充足可能性問題という。\nこの問題の最も一般的な形は、NP完全であることが知られている。\n例: $(x_1 \\lor x_2) \\land (x_1 \\lor \\bar{x_2}) \\land (\\bar{x_1} \\lor \\bar{x_2})$\n解: $x_1=\\text{True}$, $x_2=\\text{False}$\nこの例のように$\\bigwedge_i \\bigvee_j x_{i,j}$で表される論理式を連言標準形といい、その各項の変数の数が2以下であるとき2-SATと呼ばれる。 2-SATは多項式時間で解けるアルゴリズムが存在する。 一方で項の変数の数が3以下の3-SATはNP完全であることが知られている。\n問題の生成 3-SAT問題はDIMACS CNFで記述される。\nc example DIMACS-CNF 3-SAT p cnf 3 5 -1 -2 -3 0 1 -2 3 0 1 2 -3 0 1 -2 -3 0 -1 2 3 0 行頭がcの行はコメントである。 行頭がpの行には変数が3つで項数が5項である。 また、 -1 -2 -3 0は$\\bar{x_1} \\lor \\bar{x_2} \\lor \\bar{x_3}$を示している。\nつぎに、この形式の問題を機械的に作る方法を説明する。 Pythonの場合は次のようになる。\nfrom numpy.random import randint, choice def generate_3sat_sample(vn, cn): output = \u0026quot;p cnf \u0026quot; + str(vn) + \u0026quot; \u0026quot; + str(cn) + \u0026quot;\\n\u0026quot; for i in range(cn): a, b, c = randint(1, vn+1, (3,)) while a == b or b == c or c == a: a, b, c = randint(1, vn+1, (3,)) s1, s2, s3 = choice([-1, 1], (3,)) a, b, c = s1 * a, s2 * b, s3 * c output += str(a) + \u0026quot; \u0026quot; + str(b) + \u0026quot; \u0026quot; + str(c) + \u0026quot; 0\\n\u0026quot; return output output = generate_3sat_sample(10, 42) filepath='input.txt' with open(filepath, mode='w') as f: f.write(output) vnが変数の数を表し、cnが項の数を表す。\nBashの場合は次のようになる。\nvn=10 cn=42 for tmp in `seq 0 $cn`; do if [[ $tmp == 0 ]]; then echo \u0026quot;p cnf\u0026quot; ${vn} ${cn} continue fi a= b= c= while [[ $a == $b ]] || [[ $b == $c ]] || [[ $a == $c ]]; do a=$(($RANDOM % $vn + 1)) b=$(($RANDOM % $vn + 1)) c=$(($RANDOM % $vn + 1)) done a=`[[ $(($RANDOM%2)) == 0 ]] \u0026amp;\u0026amp; echo $a || echo $((- $a))` b=`[[ $(($RANDOM%2)) == 0 ]] \u0026amp;\u0026amp; echo $b || echo $((- $b))` c=`[[ $(($RANDOM%2)) == 0 ]] \u0026amp;\u0026amp; echo $c || echo $((- $c))` echo $a $b $c 0 done \u0026gt; input.txt これで大量に問題が生成出来る。\n問題を解く SATは何十年も研究されている重要な問題であるため、この問題を解くためのコマンドが公開されている。 今回はminisatを用いる。 Macの場合はbrew install minisatで簡単にインストールが可能である。\n使い方は、\n$ minisat input.txt output.txt である。 output.txtに結果が書き込まれる。\n以上を組み合わせて次のようなスクリプトを作成した。\nvn=100 N=9 echo \u0026quot;vn\u0026quot; \u0026quot;cn\u0026quot; \u0026quot;i\u0026quot; \u0026quot;sat\u0026quot; for cn in `seq 10 10 1000`; do for i in `seq 0 $N`; do for tmp in `seq 0 $cn`; do if [[ $tmp == 0 ]]; then echo \u0026quot;p cnf\u0026quot; ${vn} ${cn} continue fi a= b= c= while [[ $a == $b ]] || [[ $b == $c ]] || [[ $a == $c ]]; do a=$(($RANDOM % $vn + 1)) b=$(($RANDOM % $vn + 1)) c=$(($RANDOM % $vn + 1)) done a=`[[ $(($RANDOM%2)) == 0 ]] \u0026amp;\u0026amp; echo $a || echo $((- $a))` b=`[[ $(($RANDOM%2)) == 0 ]] \u0026amp;\u0026amp; echo $b || echo $((- $b))` c=`[[ $(($RANDOM%2)) == 0 ]] \u0026amp;\u0026amp; echo $c || echo $((- $c))` echo $a $b $c 0 done \u0026gt; tmp/input.txt minisat tmp/input.txt tmp/output.txt \u0026amp;\u0026gt; /dev/null s=`head -n 1 tmp/output.txt` echo $vn $cn $i $s done done これをbash main.sh | tee stat.txtで実行し実験を行った。\n結果 変数の数(vn)が100の3-SATについて、条件の数(cn)を$10, 20, \\cdots, 1000$と変化させて解が存在するか実験を行った。\n横軸は項の数(cn)であり、縦軸は解が存在する確率を表す。(分母は10) 解の存在する確率は条件の数が400から500にかけて一気に下がることが読み取れる。 さらに詳細な実験を行った。 vn=100の場合に、cnを$400, 401, \\cdots, 450$と変化させて解が存在する確率を測定した。\n実は3-SATは変数の数(vn)に対して、条件の数(cn)が4.26倍のときに解ける確率が50%になることと、その付近で最もこの問題を解くことが難しいことが知られている。 したがってこの実験結果はそれを確かにそれを反映していることがわかる。\n更に、変数の数(vn)が10の3-SATについて、条件の数(cn)を$1, 2, \\cdots, 100$と変化させて解が存在するか実験を行った。\nvn=100の場合とあまり形が変わらず、条件の数(cn)が4.26倍のときに解ける確率が50%となっている傾向が読み取れる。\nまとめ 充足可能性問題の相転移の実験をした。 Bashの$RANDOMが遅すぎるので、実験にかなり時間がかかった。 確かに、条件が少なすぎるときには解が存在することはすぐにわかり、条件が多すぎるときには矛盾を一つ見つければ解が存在しないことがすぐにわかる。そのため、その間ぐらいが一番難しいというのは普通なことな気がする。 4.26倍というマジックナンバーを導出する方法はないのかな。 ","date":"2022-05-23","permalink":"https://derbuihan.github.io/posts/minisat_phase_transition/","tags":["SAT","相転移","MiniSat"],"title":"充足可能性問題の相転移の実験"},{"content":"先日、三角ゲームというゲームを知った。 今回はこのゲームをPytorchを用いて解いてみた。\n三角ゲームの説明 はじめに三角ゲームについて説明する。 例として$n=5$人($A, B, C, D, E$)で遊ぶ場合を説明する。\nゲームを始める前にそれぞれのプレイヤーは、自分以外の異なる２人のプレイヤーを選択する。 ここでは、$A = (B, C)$, $B = (A, C)$, $C = (B, D)$, $D = (C, E)$, $E = (C, D)$を選んだとする。 それぞれのプレイヤーは選択したプレイヤーと正三角形を作るように移動を行う。 最終的にこの条件をなるべく満たしながら最も多くの正三角形を作成することがこのゲームの目標である。\nこの例では以下のようにそれぞれのプレイヤーが移動すれば、すべての条件を満たし最大の5個の正三角形が作成出来る。\nPyTorchを使って解く このゲームをPyTorchを用いて解いてみる。\nライブラリのインポート はじめに、準備としてライブラリのインポートと保存用の関数を用意。\nimport torch from torch import norm, dot, abs from random import randrange, random import matplotlib.pyplot as plt def savefig(xs, ys, i): plt.figure(figsize=(8,8)) plt.xlim([-0.5, 1.5]) plt.ylim([-0.5, 1.5]) plt.scatter(xs, ys, marker=\u0026quot;.\u0026quot;) plt.savefig(\u0026quot;fig/fig\u0026quot;+str(i).zfill(5)+\u0026quot;.png\u0026quot;) plt.close() importでerrorが出たらpip3 install torch matplotlibする。 savefig(xs, ys, i)は、座標のリスト(xs, ys)を取得して、図を作成しfig/fig000i.pngに保存する。\nゲームの準備 次はゲームの準備を行う。\nまず条件を決定する。 今回は自分以外の異なる二人のプレイヤーをランダムに選ぶ。\n# determine conditions n = 10 # プレイヤーの人数 conditions = [] for i in range(n): a, b = randrange(n), randrange(n) while a == i or b == i or a == b: a, b = randrange(n), randrange(n) conditions.append([a, b]) conditions = torch.tensor(conditions) 最終的にconditions.shape = (n, 2)である。 conditions[i]=[a, b]はプレイヤー$i$はプレイヤー$a, b$と正三角形を作成するという条件である。\n次に、それぞれのプレイヤーの初期位置を設定する。\n# initial positons pos = torch.tensor([[0.8*random() + 0.1 for _ in range(2)] for _ in range(n)], requires_grad=True) $x$座標も$y$座標も0.1~0.9の範囲で一様分布になるように定めている。 また、後に位置を自動微分を用いて位置を調整するので、requires_grad=Trueを設定しておく。\n損失関数の定義 損失関数を定義し、この関数が最小(0)になったとき、すべての条件が満たされ正三角形が最大個数生成される。\n# define loss def loss_func(pos): loss = torch.sum(torch.where(pos\u0026gt;1, pos-1, torch.zeros(pos.shape))) loss += torch.sum(torch.where(pos\u0026lt;0, -pos, torch.zeros(pos.shape))) for i, [a, b] in enumerate(conditions): x, y, z = pos[i], pos[a], pos[b] loss += abs(dot(y-x, z-x)/norm(y-x)/norm(z-x) - 0.5) loss += abs(dot(z-y, x-y)/norm(z-y)/norm(x-y) - 0.5) loss += abs(dot(x-z, y-z)/norm(x-z)/norm(y-z) - 0.5) return loss はじめの二行は座標が0-1の範囲に収まるように加えている。 あとは条件の三角形のすべての角度が60°になるように設定している。\n最適化 すべてのプレイヤーの位置posを、損失関数loss_funcが最小になるように最小化して最適化を行う。\n# train trace = [] # to trace loss opt = torch.optim.Adam([pos], lr=0.01) for i in range(100): xs = pos[:, 0].detach().numpy() # to numpy ys = pos[:, 1].detach().numpy() # to numpy savefig(xs, ys, i) # save fig opt.zero_grad() loss = loss_func(pos) trace.append(loss.item()) # trace loss loss.backward() opt.step() Adamを用いてposを最適化する。 loss_funcがPyTorchの関数のみで構成されているため、posの自動微分を行うことができ効率的に最小化出来る。\n可視化 traceとposを可視化する。\n# plot trace plt.plot(trace) # plot pos plt.figure(figsize=(4,4)) plt.xlim([-0.5, 1.5]);plt.ylim([-0.5, 1.5]); xs = pos[:, 0].detach().numpy() ys = pos[:, 1].detach().numpy() plt.scatter(xs, ys, marker=\u0026quot;.\u0026quot;) conditions = [[9, 8],[8, 6],[7, 9],[2, 1],[6, 3],[6, 1],[0, 2],[8, 1],[2, 7],[6, 4]]\nまた、fig/に保存した画像たちは$convert fig/fig*0.png out.gifを用いてgif動画に変換出来る。\nその他の結果 $n=10$の場合と$n=100$の場合に実験を行い、動画の作成してYouTubeにアップロードした。\nn=10の例 n=100の例 この動画の$n=10$の場合は上手に最適化されてすべての条件を満たしている、$n=100$の場合は何がなんだかよくわからない。\n感想 自動微分を用いて三角ゲームの最適化を行った。 今回の問題は必ず答えがあるわけではないので結果は微妙になってしまったが、損失関数を調整することで目に見えるいい結果が得られるかもしれないと思った。 自動微分は機械学習のモデルを最適化するためによく使われているが、他にも色々な遊び方がありそうだと思ったのでこういうゲームを解いてみた。 微分可能プログラミングも勉強してみたい。 ","date":"2022-05-15","permalink":"https://derbuihan.github.io/posts/pytorch_triangle_game/","tags":["pytorch","python"],"title":"PyTorchを用いて三角ゲームを解いてみた。"},{"content":"全結合層のネットワークやCNNを用いてMNIST画像認識の実験をした。\n実験環境 tensorflow v2.4.0 optimizer=\u0026lsquo;adam\u0026rsquo;, loss=\u0026lsquo;sparse_categorical_crossentropy\u0026rsquo; MNISTのデータの中身 MNISTは0~9の手書き文字とそのラベルのデータセットである。 6万枚のtrainデータと1万枚のtestデータに分かれている。\nこの記事では、次のように前処理を行った。\nimport tensorflow as tf (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data() X_train = X_train.astype('float32') / 255 X_test = X_test.astype('float32') / 255 0~255の値を255で割ることで0~1の値にすることによって、勾配消失を防ぐことが出来る。\n普通に学習 全結合のモデル (その1) 下記のような全結合が2層のモデルを作った。\nmodel = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28,)), tf.keras.layers.Dense(512, activation='relu'), tf.keras.layers.Dropout(0.25), tf.keras.layers.Dense(10, activation='softmax') ]) 全パラメータの数は407,050。\n学習条件はこんな感じ\nmodel.fit(X_train, y_train, epochs=50, validation_split=0.1, batch_size=32) 学習履歴と最終的な正解率と損失関数の値は次のようになった。\nTrain, loss: 0.0102, accuracy: 0.9975 Test, loss: 0.1410, accuracy: 0.9847 Testデータに対する正解率は98.47%になった。 validationの損失関数がepoch=5で最小になり、epoch=50はその2倍ぐらいになっている。\n全結合のモデル (その2) 中間層512から2048に増やした。\nmodel = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28,)), tf.keras.layers.Dense(2048, activation='relu'), tf.keras.layers.Dropout(0.25), tf.keras.layers.Dense(10, activation='softmax') ]) 全パラメータの数は1,628,170。\nmodel.fit(X_train, y_train, epochs=50, validation_split=0.1, batch_size=32 ) 学習結果は次のようになった。\nTrain, loss: 0.0144, accuracy: 0.9975 Test, loss: 0.2275, accuracy: 0.9833 パラメータ数を増やしても正解率は上がっておらず、むしろtestデータに対する正解率は下がっている。 過剰適合していることが考えられる。\n畳み込みのモデル (その1) 下記のように全結合層の前にConvとMaxPool層を入れたモデルを作成した。\nmodel = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(16, (5, 5), padding='same', input_shape=(28, 28, 1), activation='relu'), tf.keras.layers.MaxPool2D(pool_size=(2, 2)), tf.keras.layers.Flatten(), tf.keras.layers.Dense(512, activation='relu'), tf.keras.layers.Dropout(0.25), tf.keras.layers.Dense(10, activation='softmax') ]) 全パラメータの数は1,611,690。\nmodel.fit(X_train, y_train, epochs=50, validation_split=0.1, batch_size=32 ) 学習結果は次のようになった。\nTrain, loss: 0.0027, accuracy: 0.9994 Test, loss: 0.1221, accuracy: 0.9879 損失関数の収束速度が全結合に比べて速くなった。 また、損失関数が良く小さくなった。\n畳み込みのモデル (その2) 下記のように全結合層の前にConvとMaxPool層を複数積み重ねた入れたモデルを作成した。\nmodel = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1), activation='relu'), tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'), tf.keras.layers.MaxPool2D(pool_size=(2, 2)), tf.keras.layers.Dropout(0.25), tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'), tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'), tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'), tf.keras.layers.MaxPool2D(pool_size=(2, 2)), tf.keras.layers.Dropout(0.25), tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'), tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'), tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'), tf.keras.layers.MaxPool2D(pool_size=(2, 2)), tf.keras.layers.Dropout(0.25), tf.keras.layers.Flatten(), tf.keras.layers.Dense(512, activation='relu'), tf.keras.layers.Dropout(0.25), tf.keras.layers.Dense(10, activation='softmax') ]) 全パラメータの数は1,066,410。\nmodel.fit(X_train, y_train, epochs=50, validation_split=0.1, batch_size=32 ) 学習結果は次のようになった。\nTrain, loss: 0.0374, accuracy: 0.9899 Test, loss: 0.0138, accuracy: 0.9934 Dropout層が多いからなのかTrainのaccがValのaccより低くなっている。 またなぜか、学習を進めると微妙にtrainデータの損失関数の値が上昇してる気がする。\n結果まとめ モデル パラメータ数 train loss train acc test loss test acc 全結合1 407,050 0.0102 0.9975 0.1410 0.9847 全結合2 1,628,170 0.0144 0.9975 0.2275 0.9833 畳み込み1 1,611,690 0.0027 0.9994 0.1221 0.9879 畳み込み2 1,066,410 0.0374 0.9899 0.0138 0.9934 ラベルをシャッフル さて、ラベルをシャッフルして学習してみる。 Tensorflowでは次のようにすることでモデルをシャッフル出来る。\ny_train_shuffle = tf.random.shuffle(y_train) こんな感じでシャッフル出来た。\n全結合のモデル (その1) 今回は学習に時間がかかるのでepoch数を増やした。\nmodel.fit(X_train, y_train_shuffle, epochs=500, validation_split=0.1, batch_size=32) 学習履歴と最終的な正解率と損失関数の値は次のようになりました。\nTrain, loss: 1.2545, accuracy: 0.5025 Test, loss: 8.9777, accuracy: 0.1138 結果としてtrainデータに関しては50%で当てられるぐらいまで学習出来ました。 これは、すべてのtrainデータ(5.4万)の画像とラベルの対応をすべて覚えるという作業を50%ぐらいこなせるぐらい自由度の高いモデルであるということを意味しています。 また、画像とラベルに関係はないので、validationデータに関する正解率が10%になっています。 さらに、validationデータの損失関数がepochに伴って大きくなっています。\n全結合のモデル (その2) model.fit(X_train, y_train_shuffle, epochs=500, validation_split=0.1, batch_size=32) 学習結果は次のようになった。\nTrain, loss: 0.7056, accuracy: 0.7322 Test, loss: 8.0285, accuracy: 0.1152 畳み込みのモデル (その1) model.fit(X_train, y_train_shuffle, epochs=500, validation_split=0.1, batch_size=32) 学習結果は次のようになった。\nTrain, loss: 0.0328, accuracy: 0.9918 Test, loss: 13.3561, accuracy: 0.1016 全結合のモデルに比べて学習が速い。 さらに全部の画像を暗記するという問題に関しても99.18%で成功している。\n畳み込みのモデル (その2) model.fit(X_train, y_train_shuffle, epochs=500, validation_split=0.1, batch_size=32) 学習結果は次のようになった。\nTrain, loss: 2.3015, accuracy: 0.1102 Test, loss: 2.3010, accuracy: 0.1135 ほぼ学習が進んでない。 Dropout層を消しても学習は進まなかった。\n結果まとめ モデル パラメータ数 train loss train acc test loss test acc 全結合1 407,050 1.2545 0.5025 8.9777 0.1138 全結合2 1,628,170 0.7056 0.7322 8.0285 0.1152 畳み込み1 1,611,690 0.0328 0.9918 13.3561 0.1016 畳み込み2 1,066,410 2.3015 0.1102 2.3010 0.1135 ","date":"2021-01-03","permalink":"https://derbuihan.github.io/posts/mnist_experiment/","tags":["tensorflow","機械学習"],"title":"MNIST画像認識の実験"},{"content":"Tensorflow Datasetsの中から英語の文章のデータセットをまとめます。\nデータの一覧の取得 以下のコマンドでtensorflow_datasetsから取得出来るデータセットの一覧がわかります。\n\u0026gt;\u0026gt;\u0026gt; import tensorflow_datasets as tfds \u0026gt;\u0026gt;\u0026gt; tfds.list_builders() 英語のデータセット 大量のテキストのデータセット\nc4 Webクロールで集めた巨大なデータセット。英語のは約1TBある。 英語以外の言語もデータがあり、すべての言語合わせると26.76TBあるらしい。\nlibrispeech_lm 5GBのデータセット\nlm1b 5GBのデータセット\npg19 古い本のデータセット。10GBぐらい。\nreddit_disentanglement redditのデータセット。\nwiki40b wikipediaのデータセット。きれいになってる。\nwikipedia wikipediaのデータセット。きれいになってない。\nその他データセット その他の使えそうなデータセット\nag_news_subset ニュース記事のタイトルと説明文が、その記事の種類にラベル付けされたデータセット。\nmath_dataset 数学に関する英語の問題とその答えのデータセット\ntiny_shakespeare シェイクスピアの文章\n","date":"2020-12-23","permalink":"https://derbuihan.github.io/posts/tensorflow_datasets_text/","tags":["Tensorflow","Datasets","機械学習"],"title":"Tensorflow Datasetsにある英語の巨大なテキストのデータセット"},{"content":"はじめに 落ちてたi7 870のパソコンにグラボを2枚刺して、機械学習専用サーバーを作りました。 このとき、最新のTensorflowをインストールしても使えないので苦労しました。 これはi7 870はSandy Bridge以前のCPUなのでAVX命令がないのが原因です。 この記事ではi7 870でも機械学習が出来るように、少し古めのTensorflowの環境を構築する方法を解説します。\nこの情報は2020年12月に出来た方法です。\n前提 CUDA 10.2 Nvidia Driver Version: 440.100 Anacondaをインストール tensorflow v1.5.0のインストール方法 はじめに、仮想環境の作成\n$ conda create -n tf1 python==3.7 anaconda $ conda activate tf1 tensorflow-gpu v1.15.0を指定してインストール\n$ conda install -c anaconda tensorflow-gpu==1.15.0 $ conda install -c anaconda keras ついでに、tensorflow-datasetsをインストール(condaで入るのはバージョンが古いのでpipでインストール)\n$ conda install tqdm $ pip install tensorflow-datasets==3.2.1 tensorflow v2.0.0のインストール方法 はじめに、仮想環境の作成\n$ conda create -n tf2 python==3.7 anaconda $ conda activate tf2 tensorflow-gpu v2.0.0を指定してインストール(これより新しいバージョンは入らなかった)\n$ conda install -c anaconda tensorflow-gpu==2.0.0 $ conda install -c anaconda keras ついでに、tensorflow-datasetsをインストール(condaで入るのはバージョンが古いのでpipでインストール)\n$ conda install tqdm $ pip install tensorflow-datasets==3.2.1 MNISTのサンプルコード import tensorflow as tf import tensorflow_datasets as tfds print(tf.__version__) mnist = tfds.load('MNIST', data_dir=\u0026quot;./data\u0026quot;, shuffle_files=True, as_supervised=True) train, test = mnist['train'].shuffle(buffer_size=1024).batch(64), mnist['test'].batch(64) model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(512, activation=tf.nn.relu), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation=tf.nn.softmax) ]) model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy']) model.fit(train, epochs=5) result = model.evaluate(test) dict(zip(model.metrics_names, result)) 感想 最新のTensorflowをソースからビルドしたほうがいいかもしれない。 あと、Google Colab使ったほうがよい。\n","date":"2020-12-18","permalink":"https://derbuihan.github.io/posts/tensorflow_i7_870/","tags":["環境構築","tensorflow","機械学習"],"title":"i7 870にTensorflowの環境構築方法"},{"content":"私はこれまでにいくつかVPSを借りて運用してきました。 実際に借りて運用したVPSのレビューを書きます。\nScaleway CPU: 2Core x86 64bit メモリ: 2G SSD: 50GB 値段: 月3ユーロ (約400円) 場所: ヨーロッパ のプランを2017年 ~ 2019年にかけて2年近く借りてました。 初めて自ら借りたVPSということもあり長く借りていましたが、あんまり使っていないことに気づき解約しました。 場所がヨーロッパということもあり、SSHしてコマンドを入力すると遅さを感じますが、Webサービスを運用する分には問題ないと思います。 この価格帯でメモリ2GB、ストレージがSSD、通信が無制限なVPSは安いと思います。 mastodonやGitLabみたいなメモリをいっぱい使うサービスを運用するのにはオススメです。 ScalewayではCPUがARMのVPSも貸していて、それも短い期間借りていた事がありますが、dockerでサービスを運用したい場合はimageが動かないので苦労したのでやめました。\nGCE CPU: 2Core x86 64bit メモリ: 0.6GB HDD: 30GB 値段: 無料 場所: アメリカ西海岸 のプランを一年前から現在まで借りてます。 このプランはGoogleが無料でVPSを貸しているということで有名です。 私はwordpressでブログを運用しています。 メモリが0.6GBということで少ないと感じるかもしれませんが、Swapをきちんと確保すれば安定して運用できます。 通信は月に1GBまでしか無料にならないですが、CDN等を活用することでなんとかなってます。 2020年からGlobal IPを取るとお金がかかるようになるらしいので、無料じゃなくなるかもしれません。\nまとめ メモリがなくても、通信が制限されててもなんとかなります。 お金があるなら、さくらVPSを借りるのがいいと思います。\n","date":"2020-01-04","permalink":"https://derbuihan.github.io/posts/vps_review/","tags":["VPS","review"],"title":"これまでに使ってきたVPSのレビュー"},{"content":"はじめに Docker 19.03以降からDockerのコンテナからNvidia GPUを使いやすくなりました。 tensorflow-gpuをDockerで使う場合は、公式のtensorflow/tensorflow:latest-gpu-py3があります。\nエラーの内容 このコンテナをSandy Bridge以前のCPUで使うと\nIllegal instruction (core dumped) というエラーが出ます。 これはAVXという命令が古すぎるCPUにはないために起こるエラーです。\n解決方法 解決方法は\ntensorflowをコンテナ内でビルドする 古いCPUでも動くtensorflowをどっかから取ってくる。 の二種類があります。\nこの記事では2.を使い、Anacondaのリポジトリから取ってきたtensorflow-gpuを用いたDockerfileを書いたので紹介します。\n使い方 Dockerfileを使ってください。\n動作環境 Ubuntu 18.04 docker 19.03 以降 nvidia-driver-435 遊び方 環境がきちんと整っていれば次で動作すると思います。\n$ git clone https://github.com/derbuihan/docker-tensorflow-gpu $ cd docker-tensorflow-gpu $ docker build -t mnist_sample . $ docker run --gpus device=0 -it --rm mnist_sample /bin/bash (base)$ conda activate tf-gpu (tf-gpu)$ wget https://raw.githubusercontent.com/keras-team/keras/master/examples/mnist_cnn.py (tf-gpu)$ python mnist_cnn.py ","date":"2019-10-03","permalink":"https://derbuihan.github.io/posts/docker_tensorflow_gpu/","tags":["docker","tensorflow","gpu","機械学習"],"title":"tensorflow-gpuをDockerで動かしたときにIllegal instructionと出たときの対処法"},{"content":"はじめに 物理を勉強していくうちに数学にも興味をもったので、学部3年生の後半から学部4年生になるまでの約半年間ひとりで数学を勉強しました。 数学科の人からすればまだ入門したてのペーペーでしょうが、独学で数学の勉強を始める方への助けになればと思い勉強法などを書いておきます。\n数学の勉強を始める前に 数学の勉強を始める前に必ず知ってないといけないことがあります。 それは、必ず全ての証明を手で追わないといけないということです。 これをやらないと結局は定義も理解できませんし、勉強も長続きしません。\n証明を手で追うためにはいくつかの必要な知識があります。 私は学部2年の頃にこれらの知識がなく数学の勉強を始めたので挫折しました。 まずはこれらのことを知ってから数学の本を読みましょう。\n数学は論理で出来ている。 実は数学の勉強は論理学を勉強してからでないと始められません。私は文系の授業で論理学を学ぶ機会があり、そこで数学は論理学で出来ていることを知りました。論理学を勉強すると、数学の本に載っている定理もかなりのものが考えれば証明できるようになります。本としては前に書いた情報科学における論理とかでも良いかもしれませんがすこし難しいし、ここまでの知識は数学では求められていかもしれません。パソコンに詳しい人はこのpdfでCoqから論理学を勉強するのがおすすめです。\n本を探すところから始まる。 たとえば、証明の書き方には本によって癖があります。\nこの定理を示すためには、〇〇と✗✗が必要である。 〇〇を示す。 〜〜〜 〇〇が示された。 ✗✗を示す。 〜〜〜 ✗✗が示された。 よって定理が示された。 と丁寧に書いてくれる本もあれば、\n〜〜〜 〇〇が示された。 〜〜〜 ✗✗が示された。 〇〇と✗✗からこの定理が示された。 と淡白に書いてある本もあります。 かなり勉強していけばどちらで書いてあっても変わらないと感じるのかもしれませんが、はじめのうちは後者のように書かれるとすごく難しく感じます。\nまた、例が多すぎてなかなか前に進めない本や、つながりがわかりにくい本などいろいろな本があります。 私は一つの単元ごとに本を探すだけで3日ぐらいは時間を使っても良いと思ってます。 それぐらいには本選びは大事です。\nたとえば集合・位相入門 松坂は有名ですが、独学で数学を一から始めるのであればこの本はおすすめしません。さっきの例でいうと後者のような証明の書き方をしていて難しく感じるからです。\n勉強したこと 偉そうにいろいろ書きましたが、結局半年で何を勉強したのかと言いますと\n集合と位相 代数系の最初の方 関数解析をフーリエ級数展開まで テンソル解析の最初の方 複素解析の復習 を、この順番に勉強しました。 最後の3つは物理学科の院試に向けて勉強した感じです。\n一覧にするとそんなにやってない感じしますね\u0026hellip; まあ、実際ペーペーなんですけど\u0026hellip;\n読んだ本 万人におすすめできるわけではないです。 こんな本もあるよって紹介です。 でも、この記事で紹介する本は全て私が数十時間かけて読んだ本です。\n集合論 独学と自慢してましたが実は集合論だけは授業と演習の単位を取りました。 多分、どの大学でも集合と位相１みたいな授業は集合の授業のはずなので、それを取ると数学の勉強のやり方やリアルな研究者を見れて良いと思います。 集合論は数学の勉強を始めるなら一番はじめにやるべきです。\n位相空間 位相は位相入門―距離空間と位相空間を読みました。 内容は絞っているように見えますが、証明はわかりやすく書いてあるので読みやすいです。 この本に書いてある以上の知識が求められる機会が今後の勉強であるのかは謎。 第二章は定理の番号がズレているので悲しい気持ちになります。\n代数系の最初の方 代数系は代数入門―群と加群 を第2章まで読みました。 例が少なくて証明がわかりやすく書いてあるこの本にしました。 代数系は例が大事な気がするので別の本で補強しないといけない気がしてます。\nテンソル解析をすこし テンソル解析はテンソル解析を飛ばし飛ばし途中まで読みました。 特殊相対性理論でそこそこ知っているつもりでしたが、記号の意味とかを正確に知れてよかった。 多分、数学科じゃなくて物理学科の人に向けて書いた本だと思う。\n関数解析をフーリエ級数展開まで 関数解析入門を第3章まで読みました。 この本はルベーグ積分とか知らなくても読めます。 この本でヒルベルト空間の意味を知りました。 また、微分方程式を解くときに自分を騙さなくてすむようになりました。\n複素解析の復習 複素関数入門と新版 複素解析を読みました。 二年生のはじめに複素解析を勉強したときはつまらないと思いましたが、色々勉強したあとだとすごい面白いと感じるようになりました。 複素解析って位相で勉強したことも代数系で勉強したことも関数解析で勉強したことも使えるのがすごいですね。\n今後の展望 まだまだ勉強したいことはあります。\n多様体 ルベーグ積分 関数解析 この辺をやってゴリゴリになりたいとも思いますし、\n確率と統計 グラフ理論とかアルゴリズム みたいな明日にも役に立つ分野も勉強してみたいとも思います。\nまあ、最近はまたプログラミングしたい気分になってきたので、Railsとか勉強してバイトしたいなとも思っています。\n感想 数学を勉強すると一日に5ページしか進まないとか普通なので、自分がいかに無力かを知ることが出来ます。 物理で計算しているときの自分を騙している感覚を消すことが出来るので、数学を勉強することは結構おすすめです。 日本語の本になっていれば独学で勉強出来るってことがわかったのもいい経験でした。研究では論文に食らいついて勉強しないといけないから、もっとレベルの高いことが求められています。 数学、もっとなんかの約に立たないかな\u0026hellip; ","date":"2019-07-07","permalink":"https://derbuihan.github.io/posts/study_math/","tags":["数学"],"title":"物理学科が半年ぐらい数学を独学で勉強した話"},{"content":"Elmでフーリエ級数展開やライフゲームを制作したのでCI/CDを試してみました。 この記事ではElmで作ったものをGitLab CI/CDを使ってGitLab Pagesにdeployする方法を紹介します。\nコードを書く $ elm init $ vim src/Main.elm 今回はsrc/Main.elmを変換して公開します。\ngit管理する GitLabのアカウントを持ってない人はアカウントを作ってください。\n$ git init $ gibo dump elm \u0026gt; .gitignore $ git add elm.json src/Main.elm .gitignore $ git commit -m \u0026quot;first commit\u0026quot; $ git remote add origin git@gitlab.com:\u0026lt;username\u0026gt;/\u0026lt;reponame\u0026gt;.git $ git push --set-upstream origin master giboは.gitignoreを自動生成するコマンドです。(参考) .gitlab-ci.ymlを作成 $ vim .gitlab-ci.yml 以下を.gitlab-ci.ymlにコピペする\nimage: node:alpine pages: cache: paths: - node_modules/ stage: deploy script: - npm install -g --unsafe-perm elm - elm make src/Main.elm --optimize --output=public/index.html artifacts: paths: - public only: - master あとは\n$ git add .gitlab-ci.yml $ git commit -m \u0026quot;add .gitlab-ci.yml\u0026quot; $ git push src/Main.elmの内容がindex.htmlに変換され、\nhttps://\u0026lt;username\u0026gt;.gitlab.com/\u0026lt;reponame\u0026gt;/ に公開されます。\n公開されるまで少し時間がかかります。(10分ぐらい？) 以降はmasterにcommitしてpushするだけで自動でGitLab Pagesにdeployされます。\n感想 シェルスクリプト書いてやってたことがサーバー側のコンテナでできるのは便利。 テストを書きながら開発するのもElmで試したい。 Elmはアーキテクチャが決まってるから経験がなくても書きやすい。 Haskellから難しさを取り除きつつ、Haskellの書きやすさを残したいい感じの言語だと思った。 参考サイト Elmの公式ガイド: An Introduction to Elm Elmの公式ガイド(和訳): Elm について (はじめに) この記事の元ネタ: How to run an elm project from gitlab? ","date":"2019-04-08","permalink":"https://derbuihan.github.io/posts/elm_gitlab_cicd/","tags":["Elm","GitLab"],"title":"Elmで作ったものをGitLab CI/CDをつかってGitLab Pagesに公開する"},{"content":"いつものように\n$ brew update \u0026amp;\u0026amp; brew upgrade してたらcoqのversionが8.9.0になってしまい、今まで書いていたコードが動かなくなったので、coqのversionを8.8.2にします。\nほとんど↓の記事を見ながらやりました。\n参考: Homebrewで旧バージョンのパッケージをインストールしたい\nbrewでバージョン指定してインストールする方法 ローカルにCoqがある場合 $ brew info coq を実行してversion 8.8.2がローカルに残っているなと思ったら\n$ brew switch coq 8.8.2 でcoqのversionを変えれるらしいです。\nローカルにCoqがない場合 まず、\n$ cd /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core/Formula $ git log coq.rb で適当に指定のversionを見つける。\ncommit ef92c34e061cb99920f7ae05d3ba205fccc5f1b8 Author: BrewTestBot \u0026lt;homebrew-test-bot@lists.sfconservancy.org\u0026gt; Date: Wed Oct 31 17:08:20 2018 +0000 coq: update 8.8.2 bottle. 今回は、↑のversionにdowngradeしたいと思います。\n$ git checkout ef92c34e061cb99920f7ae05d3ba205fccc5f1b8 $ brew unlink coq $ HOMEBREW_NO_AUTO_UPDATE=1 brew install coq を実行する。\n$ brew info coq $ coq --version 等でversionを確認してください。\n最後に、\n$ cd /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core/Formula $ git reset --hard しときましょう。\nversionのを上げたいときは、\n$ brew switch coq 8.9.0 とかすると良い。\n感想 Coq 8.9.0にしたらRequire Export qualidが上手く動かなくなったけどなんで？ ","date":"2019-02-14","permalink":"https://derbuihan.github.io/posts/coq_downgrade/","tags":["Coq","環境構築"],"title":"MacでCoqのバージョンを指定してインストール"},{"content":"はじめに SICPを読んでたらフェルマーテストについて書いてあって、ほんとにこれ速いのかやってみようと思った話です。\nソースはfermat_test.hsからダウンロード!\nフェルマーテストとは フェルマーの小定理はご存知ですよね？\n$p$を素数のとき、$a$を$p$と互いに素な整数に対して、 $$a ^ p \\equiv a \\pmod{p}$$ が成り立つ。\nこれの対偶を考えます。\n整数$n$について、$n$と互いに素な整数$a$に対して、 $$ a ^ n \\not \\equiv a \\pmod{n} $$ が成り立つとき、整数$n$は合成数である。\nこれによって、ある数が素数であることは示せないが合成数であることを速く示すことはできます。\n実装 部品を1つ1つ揃えていきます。\n互いに素 最小公倍数を求める関数を考えます。\ngcd' :: Integer -\u0026gt; Integer -\u0026gt; Integer gcd' n 0 = n gcd' n m = gcd' m (n `mod` m) これを用いて、\ngcd' n m == 1 とすれば$n, m$は互いに素です。 (Haskellには標準でgcdがあるのでそれを使っても良い)\n$a^{n-1} \\pmod{n}$を求める いわゆる、$a^7 = a * (a^3)^2 = a * (a * a^2)^2$として計算量をへらすやつを使います。 途中で$\\pmod{n}$を挟みます。\nexp_mod :: Integer -\u0026gt; Integer -\u0026gt; Integer exp_mod a n = iter a (n-1) n where iter a n base | n == 1 = a | even n = (iter a (n `div` 2) base) ^ 2 `mod` base | otherwise = a * (iter a ((n-1) `div` 2) base) ^ 2 `mod` base これを用いて、\nexp_mod a n を計算することで、$a^{n-1} \\pmod{n}$が計算できる。\n素数判定 素数判定を実装すると次のようになります。\nis_prime' :: Integer -\u0026gt; Bool is_prime' n = and $ map (\\a -\u0026gt; test n a) [2..(n-1)] where test n a | gcd' n a == 1 = (1 == exp_mod a n) | otherwise = False 最小公倍数が1でない整数が見つかったとき、すぐに素数でないことがわかります。 互いに素な整数$a$に対して$a^{n-1} \\pmod{n}$を求めて$1$と一致するか調べます。一致しなければすぐに素数でないことがわかります。 Haskellは遅延評価なのでandはFalseが見つかった瞬間にFalseを返すように出来てるはずです。 この実装では$n$未満のすべての自然数に対して、フェルマーテストを実行しています。 この方法では素数$p$に対しては、$2 \\sim (p-1)$で割っているので完全に素数を判定することが出来ています。 [2..(n-1)]の部分をランダムな自然数に変更することで、フェルマーテストになります。(Haskellの乱数めんどくさいのでやりません。)\nまとめると これまでのコードをまとめると\ngcd' :: Integer -\u0026gt; Integer -\u0026gt; Integer gcd' n 0 = n gcd' n m = gcd' m (n `mod` m) exp_mod :: Integer -\u0026gt; Integer -\u0026gt; Integer exp_mod a n = iter a (n-1) n where iter a n base | n == 1 = a | even n = (iter a (n `div` 2) base) ^ 2 `mod` base | otherwise = a * (iter a ((n-1) `div` 2) base) ^ 2 `mod` base is_prime' :: Integer -\u0026gt; Bool is_prime' n = and $ map (\\a -\u0026gt; test n a) [2..(n-1)] where test n a | gcd' n a == 1 = (1 == exp_mod a n) | otherwise = False となる。\n遊ぶ Haskellの素数ライブラリをinstallする。\ncabal install primes $2^{67}-1$を判定 さて、$ 2^{67} - 1 = 147573952589676412927$は素数ではありません。 (参考:メルセンヌ数)\nHaskellのライブラリで素数か確認すると\nghci\u0026gt; import Date.Numbers.Primes ghci\u0026gt; isPrime $ 2^67 - 1 処理が終わりません。($\\sqrt{n}$までprimesで割っていく実装とここに書いてある)\nしかし、今回つくったis_prime'では\nghci\u0026gt; is_prime' $ 2^67 - 1 False 一瞬で処理が終わり合成数であることがわかります。(すごい)\nこのアルゴリズムのすごいところは、素因数分解は出来ないけど合成数であることはすぐに分かるところですね。 判定したい数$n$の $$ 2^{n-1}, 3^{n-1} \\cdots \\pmod{n} $$ を計算するしていって1にならないのを一つ見つけるだけで良い。\nメルセンヌ数探し primes' :: [Integer] primes' = 2: filter is_prime' [3,5..] is_prime'' :: Integer -\u0026gt; Bool is_prime'' n = and $ map (\\a -\u0026gt; test n a) $ takeWhile (\u0026lt; n) [2,3,5] where test n a | gcd' n a == 1 = (1 == exp_mod a n) | otherwise = False を追加します。この関数は2,3,5の倍数でない数に対して、\n$$ 2^{n-1}, 3^{n-1}, 5^{n-1} \\pmod{n} $$\nを計算します。 (これによってis_prime''でTrueになったとしても素数でない数が存在することになります。)\nメルセンヌ数とは、\npを素数とし$M_p = 2^p-1$もまた素数になる場合、素数$p$をメルセンヌ数といいます。\n早速探してみましょう。(1分程度動かしました。)\nghci\u0026gt; filter (\\p -\u0026gt; is_prime'' $ 2^p -1) primes' [2,3,5,7,13,17,19,31,61,89,107,127,521,607,1279,2203,2281,3217,4253,4423 実際のメルセンヌ数は次の通りです。(参考:メルセンヌ数)\np = 2, 3, 5, 7, 13, 17, 19, 31, 61, 89, 107, 127, 521, 607, 1279, 2203, 2281, 3217, 4253, 4423, 9689, .. 今回探した範囲では完全に一致してますね。(すごい)\nちなみに、Haskellの素数ライブラリで1分程度探すと\nghci\u0026gt; filter (\\p -\u0026gt; isPrime $ 2^p -1) primes [2,3,5,7,13,17,19,31 となり、31までしか探せません。\nメルセンヌ数はリュカ・テストで速くきちんとした素数判定ができるらしい。\nカーマイケル数 少し気になるのがis_prime''を通過するけど、実際に素数でない数はどんなかずなのか？ですよね。 実際に探してみましょう。\nghci\u0026gt; filter (not . isPrime) $ filter is_prime'' [2..] [1729,2821,6601,8911,15841,29341,41041,46657,52633,63973,75361,101101 となり、10万以下に11個あります。(結構少ない)\nこれらの数$n$は、$2, 3, 5$と互いに素であり、 $$ 2^{n-1}, 3^{n-1}, 5^{n-1} \\equiv 1 \\pmod{n} $$ を満たす合成数である。\nじゃあ、合成数$n$について、nと互いに素な任意のｍに対して $$ m^{n-1} \\equiv 1 \\pmod{n} $$ が成り立つような整数を探したくなります。 (このような合成数$n$をカーマイケル数といいます。)\nこのような数はフェルマーテストでは弾くことは出来なくて、実際に素因数を見つけて弾くしかありません。\nカーマイケル数を実際に探してみましょう。\nimport Data.Numbers.Primes hiding (primes') -- プログラムの最初に追加 is_carmichael :: Integer -\u0026gt; Bool is_carmichael n = and $ ((not . isPrime) n):(map (\\a -\u0026gt; test n a) [2..(n-1)]) where test n a | gcd' n a == 1 = (1 == exp_mod a n) | otherwise = True not . isPrimeを追加して素数でない数から探します。 is_primeをotherwise = Trueとするだけで良い。(フェルマーテストのみを通過できる数を探せる。) ghci\u0026gt; filter is_carmichael [2..] [561,1105,1729,2465,2821,6601,8911,10585,15841,29341,41041,46657,52633,62745,63973,75361,101101 10万以下に16個あった。\nこれらの数はたまたま約数でテストしない限りフェルマーテストで弾くことができない。(憎い)\n並列化 Haskellの並列計算については、あんまりわかってないけど適当に実装してみた。\nimport Control.Parallel.Strategies これ以降、コンパイルは\nghc -O2 fermat_test.hs -threaded -rtsopts で行い、実行は\ntime ./fermat_test +RTS -N4 で行う。\n-sをつけると詳細が見れる。 -N4はスレッド数 実装 filterの部分で並列にする。\nparFilter :: (a -\u0026gt; Bool) -\u0026gt; [a] -\u0026gt; Eval [a] parFilter f [] = return [] parFilter f (a:as) = do b \u0026lt;- rpar (f a) bs \u0026lt;- parFilter f as return (if b then a:bs else bs) main :: IO () main = do let ps = takeWhile (\u0026lt; 5000) primes' mersenne = runEval $ parFilter (\\n -\u0026gt; is_prime'' (2^n - 1)) ps print mersenne rparでタスクが作られるらしい。(タスクの振り分けはHaskellが勝手にやってくれるらしい。) 実行結果は\nreal 0m7.037s user 0m27.018s sys 0m0.313s となった。\n普通のfilterで実行すると\nreal 0m27.217s user 0m28.602s sys 0m3.278s となり、だいたい4倍も速くなった。(すごい)\n憎きカーマイケル数を探す is_carmichaelでは調べたい合成数$n$について、互いに素な全ての整数$m$に対して、 $$ m^{n-1} \\equiv 1 \\pmod{n} $$ が成り立つものを言うが、もし$m = p_1 \\cdot p_2 \\cdots$と素因数分解できるとき、 $$ p_1^{n-1} \\equiv 1, p_2^{n-1} \\equiv 1, \\cdots \\pmod{n} $$ ならば、自動的に$m^{n-1} \\equiv 1 \\pmod{n}$が成り立つから、カーマイケル数を求めるには、\n合成数$n$について、$n$未満の任意の素数$p$について、 $$ p^{n-1} \\equiv 1 \\pmod{n} $$ を計算すれば良いことになる。\nis_carmichael' :: Integer -\u0026gt; Bool is_carmichael' n = and $ map (\\a -\u0026gt; test n a) $ takeWhile (\u0026lt; n) primes where test n a | gcd' n a == 1 = (1 == exp_mod a n) | otherwise = True main :: IO () main = do let carmichael = runEval $ parFilter is_carmichael' $ filter (not . isPrime) $ takeWhile (\u0026lt;1000000) [2..] print carmichael 並列化を行う際にはrparで生成されるタスクが少ない方が良いので、parFilterに通す前に素数は弾いてしまっています。 上の考察から素数のみでテストするのはis_carmichael'で実装。(これだけで約4倍速くなった。) 実行結果は\nreal 0m5.806s user 0m10.653s sys 0m1.572s となった。 普通のfilterで実行すると\nreal 0m9.437s user 0m9.912s sys 0m1.567s となり、だいたい2倍も速くなった。\n(計算しかしてないからユーザーモードの実行時間とリアルの実行時間を見るだけで、 並列化の効率を見ることができるかも？)\n感想 素因数をみつけるという方法以外で合成数かどうか判断できるのがすごいを思った($M_{67}$を判定できたのはすごい)。 大きな素数と大きな素数の積についてフェルマーテストを行い合成数であることがわかっても、その約数はぜんぜんわかりませんっていう不思議な状態になる。 1回フェルマーテストで弾ける合成数の割合とかは気になる。また、どの数でフェルマーテストをすると弾きやすいのかとかが気になる。 並列化してパソコンに負荷をかけるのが楽しかった。(タスクが増えすぎてsegmentation faultしたりした。) カーマイケル数を効率的に探す方法とか、フェルマーテストの亜種とか、ぜんぜん違う素数判定のアルゴリズムとかを調べたら面白そうだなとおもった。 つづく\u0026hellip;\n561,1105,1729,2465,2821,6601,8911,10585,15841,29341,41041,46657,52633,62745,63973,75361,101101,115921,126217,162401,172081,188461,252601,278545,294409,314821,334153,340561,399001,410041,449065,488881,512461,530881,552721,656601,658801,670033,748657,825265,838201,852841,997633,1024651,1033669,1050985,1082809,1152271,1193221,1461241,1569457,1615681,1773289,1857241,1909001,2100901,2113921,2433601,2455921,2508013,2531845,2628073,2704801,3057601,3146221,3224065,3581761,3664585,3828001,4335241,4463641,4767841,4903921,4909177,5031181,5049001,5148001,5310721,5444489,5481451,5632705,5968873,6049681,6054985,6189121,6313681,6733693,6840001,6868261,7207201,7519441,7995169,8134561,8341201,8355841,8719309,8719921,8830801,8927101,9439201,9494101,9582145,9585541,9613297,9890881 ","date":"2019-01-29","permalink":"https://derbuihan.github.io/posts/haskell_primes/","tags":["Haskell","数学","素数"],"title":"Haskellで素数を探そうと思った話"},{"content":"目的 代数入門―群と加群 を読んでいたら自然数が加法,乗法について可換モノイドだと書いてあった気がするので、それをCoq/SSReflectで示します。 (実際はSoftware Foundationsの第一章をSSReflect使って解いただけ。)\n可換モノイド Sが可換モノイドとは\n結合律 $$ \\forall x, y, z \\in S, (x \\cdot y) \\cdot z = x \\cdot (y \\cdot z) $$ 単位元の存在 $$ \\exists e \\in S, \\forall x \\in S , e \\cdot x = x \\cdot e = x $$ 可換 $$ \\forall x, y \\in S, x \\cdot y = y \\cdot x $$ を満たすことである。\n逆元の存在 $$ \\forall x \\in S, \\exists x\u0026rsquo; \\in S, x \\cdot x\u0026rsquo; = x\u0026rsquo; \\cdot x = e $$ を加えると可換群になる。\n自然数は加法と乗法について可換モノイドであるが可換群ではない。\n準備 ssreflectをimportする。\nFrom mathcomp Require Import ssreflect. Section addition. (* 自然数は加法に関して可換モノイド *) (* ここにコードを書く *) End addition. Section multiplication. (* 自然数は乗法に関して可換モノイド *) (* ここにコードを書く *) End multiplication. 加法 結合律 Theorem plus_assoc: forall n m l : nat, (n + m) + l = n + (m + l). Proof. move =\u0026gt; n m l. elim n =\u0026gt; [//= | n' H /=]. rewrite H =\u0026gt; //=. Qed. move =\u0026gt; n m l.はintros n m l.と同じ。 elim n =\u0026gt; X.はelim n; move =\u0026gt; X.と同じ。 elim n.はinduction n.と同じ。 [//= | n' H /=]の部分は、それぞれのsubgoalに対してmove =\u0026gt;で実行する。 move =\u0026gt; //=.はreflexivity.と同じ。move =\u0026gt; /=.はsimpl.と同じ。 rewrite H =\u0026gt; //=はrewrite H; move =\u0026gt; //=.と同じ。 同じことをssreflectを使わずに書くと、\nTheorem plus_assoc': forall n m l : nat, (n + m) + l = n + (m + l). Proof. intros n m l. induction n as [|n']. reflexivity. simpl. rewrite IHn'. reflexivity. Qed. となる。\n単位元の存在 0が加法の単位元である。\nTheorem plus_iden1: forall n : nat, 0 + n = n. Proof. move=\u0026gt; //=. Qed. Theorem plus_iden2: forall n : nat, n + 0 = n. Proof. move =\u0026gt; n. elim n =\u0026gt; [//= | n' H /=]. rewrite H =\u0026gt; //=. Qed. 可換 ここで必要になる補題\nLemma plus_n_Sm: forall n m: nat, S (n + m) = n + S m. Proof. move =\u0026gt; n m. elim n =\u0026gt; [//= | n' H /=]. rewrite H =\u0026gt; //=. Qed. これを用いて示す。\nTheorem plus_comm: forall n m : nat, n + m = m + n. Proof. move =\u0026gt; n m. elim n =\u0026gt; [//= | n' H /=]. rewrite H plus_n_Sm =\u0026gt; //. Qed. 乗法 右分配法則 Lemma mult_plus_distr_r: forall n m l : nat, (n + m) * l = n * l + m * l. Proof. move =\u0026gt; n m l. elim n =\u0026gt; [//= | n' H /=]. rewrite H plus_assoc =\u0026gt; //. Qed. 結合則 Theorem mult_assoc: forall n m l : nat, (n * m) * l = n * (m * l). Proof. move =\u0026gt; n m l. elim n =\u0026gt; [//= | n' H /=]. rewrite -H mult_plus_distr_r =\u0026gt; //. Qed. 単位元の存在 1が乗法の単位元である。\nTheorem mult_iden1: forall n : nat, 1 * n = n. Proof. move =\u0026gt; n. elim n =\u0026gt; [//= | n' H /=]. rewrite plus_iden2 =\u0026gt; //=. Qed. Theorem mult_iden2: forall n : nat, n * 1 = n. Proof. move =\u0026gt; n. elim n =\u0026gt; [//= | n' H /=]. rewrite H =\u0026gt; //=. Qed. 可換 ここで必要になる補題\nLemma mult_Sn: forall n m : nat, n + n * m = n * S m. Proof. move =\u0026gt; n m. elim n =\u0026gt; [//= | n' H /=]. rewrite -H -2!plus_assoc (plus_comm n' m) =\u0026gt; //. Qed. これを用いて示す。\nTheorem mult_comm: forall n m : nat, n * m = m * n. Proof. move =\u0026gt; n m. elim n =\u0026gt; [//= | n' H /=]. rewrite H mult_Sn =\u0026gt; //. Qed. 左分配法則 おまけ\nLemma mult_plus_distr_l: forall n m l : nat, n * (m + l) = n * m + n * l. Proof. move =\u0026gt; n m l. rewrite (mult_comm n (m + l)) (mult_comm n m) (mult_comm n l) mult_plus_distr_r =\u0026gt; //. Qed. 感想 この記事を見ながら書いた。 ","date":"2019-01-16","permalink":"https://derbuihan.github.io/posts/coq_natural_number/","tags":["Coq","数学"],"title":"Coqで自然数が加法,乗法について可換モノイド"},{"content":"直観論理とは Coqは直観論理で許される推論規則で演繹を行う。 直観論理の説明はWikipediaにある。\n簡単に説明すると、私達が普通に数学で用いている論理は古典論理で、直観論理は古典論理から排中律を除いたものである。 排中律を除くことで二重否定の除去など、古典論理では証明できるが直観論理では証明できないものがある。 CoqではClassicalというライブラリをImportすると古典論理を使えるようになる。 詳しい説明は情報科学における論理を読むと良い。\nこの記事では、直感論理と古典論理の微妙な違いを集めて遊ぼうと思います。\n直感論理で示せる命題 二重否定をつける Lemma PPNN : forall p : Prop, p -\u0026gt; ~~p. Proof. intros. intro. apply H0. trivial. Qed. 三重否定を否定にする Lemma NNN : forall p : Prop, ~~~ p -\u0026gt; ~p. Proof. intros. intro. apply H. intro. contradiction. Qed. 対偶の片方 Lemma Cont : forall p q : Prop, (p -\u0026gt; q) -\u0026gt; (~q -\u0026gt; ~p). Proof. intros. intro. apply H0. apply H. trivial. Qed. 古典論理で示せる命題 古典論理で示せる命題と真理値表を用いて恒真になる命題は一致します。(命題論理の完全性と健全性で検索)\n排中律を入れる。\nSection Classical. Axiom classic : forall P : Prop, P \\/ ~P. (* ここにコードを書く *) End Classical. 二重否定の除去 Lemma NNPP : forall p : Prop, ~~ p -\u0026gt; p. Proof. intros. specialize (classic p). intros. destruct H0. trivial. contradiction. Qed. 対偶のもう片方 Lemma Cont2 : forall p q : Prop, (~p -\u0026gt; ~q) -\u0026gt; (q -\u0026gt; p). Proof. intros. specialize (classic p). intros. destruct H1. trivial. specialize (H H1). contradiction. Qed. 背理法 apply NNPP; intros. を実行することは背理法を行うことである。 示すべき命題の否定を仮定に追加して矛盾を導くことだからである。\n感想 情報科学における論理にいろいろ書いてあった。 Require Import Classical.することで古典論理を扱うことができる。Classical 直感論理を考えることはなんかの役に立つのかな。 ","date":"2019-01-13","permalink":"https://derbuihan.github.io/posts/coq_logic/","tags":["Coq"],"title":"Coqは直観論理である。"},{"content":"Rootなしで人権を得る方法です。\nTmuxをビルドする。 こんな素晴らしい記事がある。 これをコピペして実行。\nlinuxbrewを用いる。 以下を~/.bashrcに追加して再度ログイン。\nif [ ! -r ~/.linuxbrew ]; then git clone https://github.com/Linuxbrew/brew.git ~/.linuxbrew fi if [ -r ~/.linuxbrew ]; then export PATH=\u0026quot;$HOME/.linuxbrew/bin:$HOME/.linuxbrew/sbin:$PATH\u0026quot; export MANPATH=\u0026quot;$MANPATH:$HOME/.linuxbrew/share/man\u0026quot; export INFOPATH=\u0026quot;$INFOPATH:$HOME/.linuxbrew/share/info\u0026quot; #export LD_LIBRARY_PATH=\u0026quot;$LD_LIBRARY_PATH:$HOME/.linuxbrew/lib\u0026quot; fi ","date":"2019-01-12","permalink":"https://derbuihan.github.io/posts/without_root/","tags":["環境構築"],"title":"Root権限がない環境で色々インストールする。"},{"content":"外延性の公理から集合の等号に対して同値関係を示します。(Coqで)\n外延性の公理とは A, Bを任意の集合とするとき、もし任意の集合Xについて「XがAの要素であるならば、そのときに限りXはBの要素である」が成り立つならば、AとBは等しい。\n外延性の公理から引用。\n∈と=を定義する Section Axiom_Of_Extensionality. Variable Var : Type. Variable In : Var -\u0026gt; Var -\u0026gt; Prop. Variable Eq : Var -\u0026gt; Var -\u0026gt; Prop. (* ここにコードを書く *) End Axiom_Of_Extensionality. ∈をIn、=をEqとした。\n同値関係とは 反射律: a = a 対称律: a = b ⇒ b = a 推移律: a = b ∧ b = c ⇒ a = c 同値関係を参考。\n証明する まず、外延公理を入れる。\nAxiom Extensionality: forall x y, Eq x y \u0026lt;-\u0026gt; forall z : Var, (In z x \u0026lt;-\u0026gt; In z y). 反射律 Lemma Equal_Reflexive : forall x : Var, Eq x x. Proof. intros. specialize (Extensionality x x). intros. destruct H. apply H0. intros. split. intros. trivial. intros. trivial. Qed. specialize.からintros.のパターンで公理を仮定における。 specialize.からintros.のパターンってTacticないのかな。 対称律 Lemma Equal_Symmetric : forall x y : Var, Eq x y -\u0026gt; Eq y x. Proof. intros. specialize (Extensionality y x). specialize (Extensionality x y). intros. destruct H0. destruct H1. apply H3. intros. specialize (H0 H). specialize (H0 z). destruct H0. split. trivial. trivial. Qed. \u0026lt;-\u0026gt;が対称律を満たすことを示すのと本質的にはあんまり変わらない。 先に\u0026lt;-\u0026gt;の対称律を示したら置き換えて楽に示せたら良いのに。 推移律 Lemma Equal_Transitive : forall x y z : Var, Eq x y /\\ Eq y z -\u0026gt; Eq x z. Proof. intros. specialize (Extensionality x y). specialize (Extensionality y x). specialize (Extensionality y z). specialize (Extensionality z y). specialize (Extensionality z x). specialize (Extensionality x z). intros. destruct H0. destruct H1. destruct H2. destruct H3. destruct H4. destruct H5. destruct H. apply H6. intros. split. intros. specialize (H3 H12). specialize (H3 z0). destruct H3. apply H3. specialize (H5 H). specialize (H5 z0). destruct H5. apply H5. trivial. intros. specialize (H5 H). specialize (H5 z0). destruct H5. apply H14. specialize (H3 H12). specialize (H3 z0). destruct H3. apply H15. trivial. Qed. 同じ公理を6回specialize.しないといけないもんなのかな。 長すぎてもう見返したくない。 SSReflect 外延性の公理から集合の等号に対して同値関係を示します。(SSReflectで)\nFrom mathcomp Require Import ssreflect. Section Axiom_Of_Extensionality. Variable Var : Type. Variable In : Var -\u0026gt; Var -\u0026gt; Prop. Variable Eq : Var -\u0026gt; Var -\u0026gt; Prop. Notation \u0026quot;a ∈ A\u0026quot; := (In A a) (at level 55,no associativity). Notation \u0026quot;A ≡ B\u0026quot; := (Eq A B) (at level 57, no associativity). Axiom Extensionality: forall x y : Var, x ≡ y \u0026lt;-\u0026gt; forall z : Var, (z ∈ x \u0026lt;-\u0026gt; z ∈ y). Lemma Equal_Reflexive : forall x : Var, x ≡ x. Proof. move =\u0026gt; x. elim : (Extensionality x x) =\u0026gt; H1 H2. by apply : H2. Qed. Lemma Equal_Symmetric : forall x y : Var, x ≡ y -\u0026gt; y ≡ x. Proof. move =\u0026gt; x y. elim : (Extensionality x y) =\u0026gt; H1 H2. elim : (Extensionality y x) =\u0026gt; H3 H4 XeqY. apply : H4 =\u0026gt; z. apply iff_sym. by apply : (H1 XeqY). Qed. Lemma Equal_Transitive : forall x y z : Var, x ≡ y /\\ y ≡ z -\u0026gt; x ≡ z. Proof. move =\u0026gt; x y z. elim : (Extensionality x y) =\u0026gt; H1 H2. elim : (Extensionality y z) =\u0026gt; H3 H4. elim : (Extensionality x z) =\u0026gt; H5 H6. case =\u0026gt; XeqY YeqZ. apply : H6 =\u0026gt; z0. move : (H1 XeqY z0) (H3 YeqZ z0). by apply iff_trans. Qed. End Axiom_Of_Extensionality. すごく短くなった 感想 SSReflectを使おう! ","date":"2018-12-31","permalink":"https://derbuihan.github.io/posts/coq_axiom_of_extensionality/","tags":["Coq","数学"],"title":"Coqで外延性の公理"},{"content":"随時更新していきます。\n通常編 とりあえずAtomをインストールしたら入れるプラグイン\natomic-emacs AtomでEmacsのキーバインドを使う。\njapanese-menu 設定を日本語化\nplatformio-ide-terminal Ctrl-`で下からターミナルが出てくるようになる。\nproject-manager projectを管理できるようになる。\nLaTex環境編 texをAtomで書くときのプラグイン一覧。\nlanguage-latex シンタックスハイライト\nlatex texのコンパイル\nlatexer 補完\npdf-view pdfを見れるようになる。\n感想 AtomでCoqやりたい。 AtomではVimよりEmacsのが使いやすい。 ","date":"2018-12-29","permalink":"https://derbuihan.github.io/posts/atom_package/","tags":["Atom","Tex","環境構築"],"title":"Atomで入れるプラグイン一覧"},{"content":"はじめに Coqで集合を扱う標準ライブラリEnsemblesの再開発を行います。 勉強のためauto.を使わないで証明を行います。\n集合論 Section Ensembles. (* ここにコードを書く *) End Ensembles. 集合とは 1変数の述語を用いて集合論を定義する。\nVariable U : Type. Definition Ensemble := U -\u0026gt; Prop. Uは述語の変数の型であり、EnsembleはUを受け取って命題を返す1変数の述語である。 今回はEnsembleを集合とする。\n∈を定義 元aが集合Aに含まれるということは、変数aと1変数の述語AについてA(a)ということである。\n例): 「ソクラテスが人間という集合に含まれる。」は、「ソクラテスが人間ある。」ということである。\nDefinition In (A : Ensemble) (a : U) : Prop := A a. Notation \u0026quot;a ∈ A\u0026quot; := (In A a) (at level 55,no associativity). Notation \u0026quot;a ∉ A\u0026quot; := (~ In A a) (at level 55,no associativity). Notationを定義すると楽しくなってきますね。\n排中律を示す。 その前に\nRequire Import Classical. をSectionの一番最初に追加しましょう。 (Coqは直観論理なのでClassicalをImportしないと排中律を示すことが出来ない。)\nLemma excluded_middle : forall A : Ensemble , forall a : U, (a ∈ A) \\/ (a ∉ A). Proof. unfold In. intros. apply classic. Qed. unfoldは定義を展開するTacticである。\n⊆を定義 A ⊆ Bは任意のAの元はBに含まれることである。\nDefinition Include (A : Ensemble) (B : Ensemble) : Prop := forall a : U, a ∈ A -\u0026gt; a ∈ B. Notation \u0026quot;A ⊆ B\u0026quot; := (Include A B) (at level 54, no associativity). ⊆の反射律と推移律を示す。\nLemma reflexive : forall A : Ensemble, A ⊆ A. Proof. unfold Include. intros. trivial. Qed. Lemma transitive : forall A B C : Ensemble, A ⊆ B /\\ B ⊆ C -\u0026gt; A ⊆ C. Proof. unfold Include. intros. destruct H. specialize (H a). specialize (H1 a). apply H1. apply H. trivial. Qed. auto.を使うと楽に証明できる。\n≡を定義 「A ≡ B」は「A ⊆ B かつ B ⊆ A」ということである。\nDefinition Same_set (A : Ensemble) (B : Ensemble) : Prop := A ⊆ B /\\ B ⊆ A. Notation \u0026quot;A ≡ B\u0026quot; := (Same_set A B) (at level 57, no associativity). 空集合と全体集合を定義 空集合は元が一つもないので、どんな変数を取ってもFalseを返し、\n全体集合はすべてが元であるから、どんな変数を取ってもTrueを返す。\nDefinition Empty_set : Ensemble := fun a : U =\u0026gt; False. Notation \u0026quot;∅\u0026quot; := Empty_set. Definition Full_set : Ensemble := fun a : U =\u0026gt; True. Notation Ω := Full_set. 空集合は任意の集合の部分集合であり、全体集合は任意の集合を部分集合にもつことを示す。\nLemma empty_subset : forall A : Ensemble, ∅ ⊆ A. Proof. unfold Include. intros. contradiction. Qed. Lemma subset_full : forall A : Ensemble, A ⊆ Ω. Proof. unfold Include. intros. unfold Full_set. unfold In. trivial. Qed. 仮定にFalseがあるとき任意の命題を示すことができる。 仮定にFalseがあったらcontradiction.を用いる。\n感想 ラッセルのパラドックスは型をつけると回避できるとかどっかで聞いたけどよくわからない。 Coqで公理的集合論をやりたかったけどどうやれば良いのかよくわからない。 ","date":"2018-12-27","permalink":"https://derbuihan.github.io/posts/coq_set_theory/","tags":["Coq","数学"],"title":"Coqで集合論 その1"},{"content":"Hugoとは オープンソースの軽量ブログジェジェネレーターです。 Markdownで書くことが出来て楽です。\nHugoの使い方 インストール方法 brew install hugo ブログの作成 hugo new site hugo 記事の作成 hugo new post/fuga.md localhostでテスト hugo server を実行し、http://localhost:1313/ にアクセス。 この時はpublicには何も作成されない。\nページを作成する。 hugo を実行するとpublicにサイトが作成される。\ngitで管理する方法 HUGOでブログ作成 → GitHub Pagesで公開する手順にgitのサブモジュールを用いて管理する方法が書いてある。 素晴らしいのでやると良い。\n感想 GitHub Pagesでブログができてドメイン代しかかからないのが素晴らしいと思いました。 gitでpushすると更新なのでネットがないところでもblogが書けるのもいいと思います。 Hexoもいいけどnpmで拡張するとかめんどくさいからHugoが楽で良いと思った。 テーマを変えるのも楽だけど、私のセンスがないのでこのブログはダサい。 ","date":"2018-12-20","permalink":"https://derbuihan.github.io/posts/start_blog_hugo/","tags":["Hugo"],"title":"hugoを使って軽量ブログを作る"}]